{
  "q01::strict": {
    "query": "What problem does C2LLM aim to solve in code retrieval systems?",
    "answer": "I cannot answer this reliably with the available evidence (Verification Failed).",
    "evidence": [
      {
        "paper_id": "22aa53245f6ef08b6d5380a270e0461bcf35fe98232746c006ed9bc7edd3f7de",
        "chunk_id": "22aa53245f6ef08b6d5380a270e0461bcf35fe98232746c006ed9bc7edd3f7de::sec::unknown::chunk::0",
        "section": "unknown",
        "order": 0,
        "text": "C 2. LLMTechnicalReport C 2. LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling JinQin\u2217,1 ZihanLiao\u2217,1 ZiyinZhang\u2217,1,2 HangYu\u2020,1 PengDi\u2020,1. RuiWang\u2020,2 1. AntGroup 2. ShanghaiJiaoTongUniversity 1{qj 431428,liaozihan.lzh,hyu.hugo,dipeng.dp}@antgroup.com 2{daenerystargaryen,wangrui 12}@sjtu.edu.cn \u00a7https://github.com/codefuse-ai/CodeFuse-Embeddings https://huggingface.co/collections/codefuse-ai/codefuse-embeddings 85 80 75 70 65 60 55 50 Granite-Emb G ed ra -S n - i R te 2 -Embe T d e - x R t- 2 Em IN be F d - - R 0 e 0 t 5 ri E ev m er b - e 1 d .5 G B emm I a N - F 0. - 3 R B et Q ri w ev e e n r 3 -7 -E B mbed-0. C 6. B 2. LL G M em -0 in .5 i- B Emb Q e w d e -0 n 0 3 1 -Em Q b w ed e - n 4 3 B -Embe S d e -8 e B d 1.6-Embed C 2. LLM-7. B ecnamrofreP edoC-BETM Closed-source Model 80.07 80.69 80.71 80.75 75.42 75.46 76.00 69.70 68.49 68.76 61.51 57.22 55.84 Figure 1: MTEB-Codeleaderboard. C 2. LLM-7. Branks 1. stamongallmodels,surpasssingthebest closed-sourcemodels,whileC 2. LLM-0.5 Branks 1. stamongmodelswithlessthan 1. Bparameters, and 6. thoverall."
      },
      {
        "paper_id": "4fe9d12bf408af06b1a16db45d3a4c9efb93cd4265c995051c367286a51b40d7",
        "chunk_id": "4fe9d12bf408af06b1a16db45d3a4c9efb93cd4265c995051c367286a51b40d7::sec::references::chunk::3",
        "section": "references",
        "order": 3,
        "text": "Wenhao Zeng, Yaoning Wang, Chao Hu, Yuling Shi, Chengcheng Wan, Hongyu Zhang, and Xiaodong Gu. 2025. Pruning the unsurprising: Efficient code reasoning via first-token surprisal. ArXiv, abs/2508.05988. 11 A LLMUsage Shorthand HuggingFaceIdentifier Llama 3.1-8. B meta-llama/Llama-3.1-8. B-Instruct Other than being used as part of the experiments Qwen 2.5-7. B Qwen/Qwen 2.5-7. B-Instruct conducted in this work, LLMs were used solely Llama 2-7. B meta-llama/Llama-2-7. b-chat-hf Mistral-7. B mistralai/Mistral-7. B-Instruct-v 0.3 asawritingassistancetoolinpreparingthispaper submission. Theirrolewaslimitedtopolishinglan- GSM 8. K openai/gsm 8. k MMLU-Pro TIGER-Lab/MMLU-Pro guage,improvingclarity,andreducingredundancy. MATH simplescaling/openaimath The prompt used for this purpose was similar to \u201cPlease revise the writing of this, making sure to Table 1: Mapping from shorthand model and dataset remove any grammatical mistakes.\u201d All research namestotheircorrespondingHuggingFaceidentifiers. ideas,experimentaldesigns,analyses,andclaims presentedinthepaperareentirelytheoriginalwork Model GSM 8. K MMLU-Pro MATH-500 oftheauthors. Nopartoftheconceptual,method- Llama 2-7. B 25.02 15.58 4.60 Mistral-7. B 31.46 30.28 13.40 ological, or empirical contributions relies on or Qwen 2.5-7. B 90.98 56.25 65.80 Llama 3.1-8. B 82.34 45.21 43.60 originatesfromLLMoutputs. Table 2: Zero-shot Performance. Accuracy (%) of B ImplementationDetails modelsonthetestsplitsofthedatasets. B.1 Datasets B.2 Teacher\u2013Pruner\u2013StudentConfiguration We conduct experiments on three reasoning- Weinstantiatetheteacher\u2013pruner\u2013studentdistilla- intensive benchmarks: (i) GSM 8. K (Cobbe et al., tionframeworkwithfourinstruction-tunedmodels 2021),consistingofgrade-schoolarithmeticword andthedatasetslistedinTable 1. Tojustifymodel problems; (ii) MMLU-Pro (Wang et al., 2024), role assignments, we first evaluate each model\u2019s a multi-domain benchmark spanning diverse do- zero-shot performance on the corresponding test mains or subjects; and (iii) MATH (Hendrycks splits(Table 2). Allzero-shotevaluationsusede- etal.,2021),containingOlympiad-levelmathemat-"
      },
      {
        "paper_id": "4fe9d12bf408af06b1a16db45d3a4c9efb93cd4265c995051c367286a51b40d7",
        "chunk_id": "4fe9d12bf408af06b1a16db45d3a4c9efb93cd4265c995051c367286a51b40d7::sec::unknown::chunk::0",
        "section": "unknown",
        "order": 0,
        "text": "Do LLMs Encode Functional Importance of Reasoning Tokens ? JanvijaySingh DilekHakkani-T\u00fcr TheGraingerCollegeofEngineering UniversityofIllinoisUrbanaChampaign {jvsingh 2, dilek}@illinois.edu"
      },
      {
        "paper_id": "06a2328681ecddca675125a619a76177d24c96614cf6f5bd65b3b97050e69bd9",
        "chunk_id": "06a2328681ecddca675125a619a76177d24c96614cf6f5bd65b3b97050e69bd9::sec::abstract::chunk::1",
        "section": "abstract",
        "order": 1,
        "text": "The LLM executes these instructions, and TheadventofLLMs,especiallyaftertheimpressivedemon- the best instruction improving the scoring metric is retained. strations of ChatGPT, led to a naive belief in many cus- Automatic prompt engineering is envisaged to be applied to tomers/companies that a new dawn of business intelligence any task that is solved by prompting LLMs. is on the horizon which would require very limited effort However,existingapproachesoftenrequireaninitialhuman to exploit the LLMs on their usecases. It turned out that, writtentaskspecificprompt.Theyalsoneedextensivenumber moreoftenthannot,thedownstreamperformanceofLLMsis of expensive LLM calls during the optimization process. In heavilycoupledwiththequalityofthepromptusedtoinstruct addition, they are not easily adoptable due to implementation the model. Studies have shown that large language models complexities.Also,allofthemrequireevaluationdatafortheir (LLMs) are quite sensitive to minor variations in prompt scoring metrics to work to pick the best possible prompt. phrasing. For example, the addition, removal, or reordering In this paper, we propose a system that is based on a new of just a few tokens can lead to significant differences in task simple yet effective approach for automatic prompt engineer- performance [9]. Generic prompts do not typically produce ing that uses only a few examples and controlled randomized good responses and the most effective prompts are almost sampling to generate the best possible prompt. always"
      },
      {
        "paper_id": "b470be6b294b53fc75b90b830d65b5a86ab805a30db5dac3f438f56a929c3232",
        "chunk_id": "b470be6b294b53fc75b90b830d65b5a86ab805a30db5dac3f438f56a929c3232::sec::abstract::chunk::23",
        "section": "abstract",
        "order": 23,
        "text": "Wang, Kwang-Ting Cheng, Yejin viacontextsummarization. CoRR,abs/2509.13313. Choi, Jan Kautz, and Pavlo Molchanov. 2025. DLER: doing length penalty right - incentivizing Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan moreintelligencepertokenviareinforcementlearn- Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, ing. CoRR,abs/2510.15110. and Maosong Sun. 2.024 a. InfLLM: Training-free long-contextextrapolationforllmswithanefficient Xin Liu and Lu Wang. 2025. Answer convergence contextmemory. InProceedingsofthe 3.8 thInterna- as a signal for early stopping in reasoning. CoRR, tionalConferenceonNeuralInformationProcessing abs/2506.02536. Systems,pages 119638\u2013119661. 10 GuangxuanXiao,JiamingTang,JingweiZuo,Junxian YuqiZhu,GeLi,XueJiang,JiaLi,HongMei,ZhiJin, Guo,ShangYang,HaotianTang,YaoFu,andSong andYihongDong.2025. Uncertainty-guidedchain- Han. 2025. DuoAttention: Efficient long-context of-thought for code generation with llms. CoRR, LLM inference with retrieval and streaming heads. abs/2503.15341. InProceedingsoftheThirteenthInternationalCon- ferenceonLearningRepresentations. Jacob Ziv and Abraham Lempel. 1977. A universal algorithm for sequential data compression. IEEE Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Trans.Inf.Theory,23(3):337\u2013343. Han, and Mike Lewis. 2.024 b. Efficient streaming language models with attention sinks. In Proceed- ingsofthe 1.2 thInternationalConferenceonLearn- A LZ 7.7 CompressionAlgorithm ingRepresentations. We describe the LZ 77 algorithm in detail. LZ 77 Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak operates using a sliding window buffer of fixed Shafran, Karthik R. Narasimhan, and Yuan Cao. lengthn,dividedintotwoparts: 2023. ReAct: Synergizing reasoning and acting in languagemodels. InThe 1.1 thInternationalConfer- \u2022 Asearchbufferoflengthn\u2212L ,whichholds s enceonLearningRepresentations. recently encoded data and acts as a dynamic Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, dictionary. Liang Zhao,"
      },
      {
        "paper_id": "2558d034ca1cd9d7f81165e7878b8f394ad83d7054c62940ed7e734a86d157bf",
        "chunk_id": "2558d034ca1cd9d7f81165e7878b8f394ad83d7054c62940ed7e734a86d157bf::sec::methodology::chunk::2",
        "section": "methodology",
        "order": 2,
        "text": "Specifically, ifasyntheticallygeneratedpositivequeryfromStep 1. receivedanunexpectedlylowscore(0. or 1), were-labeled itwithGPT-4. ousingthesameprompt. Ifthelowscorepersisted,weretaineditasavalidnegativecase;otherwise, wediscardedthequery. Thisfilteringprocesspreventsthemodelfromlearningcontradictorysignalsandenforcesa consistenttrainingdistribution. Aftercompletingthesefoursteps,weobtainedahigh-qualitysetofsyntheticquery\u2013document\u2013labeltriplets,which servedasthecoretrainingdatasetforfine-tuningourSLMlabeler. 4. SLMlabelerfine-tuning: Finally,wefine-tunedthePhi-3.5 MiniInstructmodelusingthelabeledquery-document pairs. Tooptimizetheinstructfine-tuningprocess, weadaptedthesameprompttemplateforLLMlabelinginthe previousstepafterremovingtheinstructionforthemodeltogenerateexplanationsalongsidethepredictedrelevance score. Through empirical analysis, we found that including explanations, though useful for interpretability, led to inconsistenttrainingsignalsandintroducedunnecessaryverbosityinthetrainedSLM\u2019soutput. Tofurtherimprovethe generalizationabilityofthefine-tunedsmalllanguagemodel,weemployedmulti-tasktuning[47]. Asillustratedin Figure 3,thisphaseincorporatedseveraladditionaldatasetstoexposethemodeltoabroaderrangeofquery-document relevancescenarios. Specifically,weinclude: \u2022 INTERS[42]: Itconsistsofdatasetsfor 1.9 differentsmalltasksaboutqueryanddocumentunderstanding. Weuseditformulti-tasktuningtoenhancemodels\u2019generalizability. \u2022 TREC-CAsT[6]: Thisdatasetconsistsofsemanticwebquery-passagepairswithhumanlabelsonscaleof 0-4. \u2022 MSMARCOpassages[5]:Itconsistsofover 4.00 Kpassagesfromvarioussources. WeuseGPT-4. otogenerate sematicqueriesunder 0-4. relevancescaleunderrandomlyselectedpassagesbasedonachain-of-thoughtidea. Althoughourprimarygoalisenterprisequerylabeling, wedeliberatelyincorporatedsemanticwebquerydatasets fromthepublicdomain. Webelievethisbroadersupervisionhelpsthemodeldevelopastrongerunderstandingof query\u2013document relevance beyond strictly keyword-matching setting, thereby improving both generalization and robustness. Specifically,asshowninFigure 3,wefirsttrainedthePhi-3.5 MiniInstructonINTERSformulti-task tuningtoenhanceitsrobustnessandoverallcapabilities,andthenwecontinuouslyfine-tunedthetrainedmodelona combinationofthreedatasetswementionedabove: syntheticenterprisequery-document-labeltriplets,TREC-CAsT withhumanlabels,andMSMARCOpassageswithsyntheticqueries. Wetrainthemodelfor 2. epochsusingamaximumsequencelengthof 4096. Theeffectivebatchsizeis 32,achievedby settingtheper-devicebatchsizeto 4. andusing 8. gradientaccumulationsteps. Traininglogsarerecordedevery 4.0 steps. Evaluationisperformedevery 8.0 stepsusingaseparatehuman-labelledtestdatasetevaluationdatasetthatdoesnot influencethetrainingprocess,asthereisnoearlystoppingorfeedbackloopfromtheevaluationset. Bothtrainingand evaluationuseabatchsizeof 4. perdevice. Thefine-tuningwasconductedonaclusterofeightA 1.00 GPUs. Itachieves atrainingspeedofapproximately 5.84 samplespersecondontheGPUcluster. 9 Figure 3: SLMlabelersupervisedfine-tuning(SFT)workflow: wefirstfine-tunedPhi-3.5 MiniInstructontheINTERS datasetformulti-taskpre-training,andthenfine-tunedontheresultingmodelagainwithacombinationofsyntheticand open-sourceddatasets. Box 3.3: RelevanceLabelingPrompt Role Youareanenterprisesearchqualityraterevaluatingfile/messagerelevance. Task Givenaqueryandanentity(document),assignarelevancescoreintherangeof 0\u20134: \u20134=idealquality,shouldbetheidealresult ... \u20130=badquality,shouldneverbeshown Input \u2013Querytext \u2013Documentmetadataandhighlights Output Score: <between 0 and 4> 4 ExperimentalResults In this section, we present a detailed evaluation of the performance of the fine-tuned Phi-3.5 Mini Instruct model comparedtothebasemodel(vanillaPhi-3.5 MiniInstruct)andGPT-4. o. Theresultsshowthatfine-tuningsignificantly enhancesthealignmentofthePhi-3.5 Minimodelwithhumanrelevancejudgments, matchingorevenmarginally surpassingGPT-4. oonbothevaluationmetrics. Fine-tuningImpact: Fine-tuninghasshownasubstantialimpactonmodelperformance. AshighlightedinFigure 4, theSLM-HumanNDCGincreasedfrom 0.815 to 0.953,reflectingasignificantimprovementinthemodel\u2019salignment withhumanjudgments. Similarly,pairwiseaccuracyincreasedfrom 42.15 to 63.81,surpassingevenGPT-4. oonboth keymetrics(NDCG:0.944,Accuracy: 62.58). ThisconfirmsthatadaptingPhi-3.5 Minitoenterpriserelevancetasks yieldsnotonlycompetitivebutsuperiorperformance,underscoringtheeffectivenessofoursyntheticdatageneration andfine-tuningpipeline. Wealsoconductedcomprehensiveablationstudiesonthetrainingdata,withresultssummarizedinTable 4. Ourkey findingsareasfollows: DataSizeImpact: Itiswidelyknownthatmoretrainingdatamayyieldbetterresults;however,ourexperiments showthatthiseffectplateausbeyondacertainpoint. Thefine-tuningresultshighlighttheroleofdatasizeinshaping modelperformance. AsshowninTable 4,increasingtheamountofsynthetictrainingdatafrom 1.4 Kto 2.4 Kexamples (row 2. vs. row 3)yieldsonlymarginalgains. With 1.4 Kexamples,fine-tuningproducedasubstantialimprovementin SLM-HumanNDCG(from 0.815 to 0.953)andpairwiseaccuracy(from 42.15 to 63.81). Expandingto 2.4 Kexamples, however,resultedinnearlyidenticalperformance(NDCG:0.954,Accuracy: 63.55). Asimilarpatternisobserved whencomparingrow 4. androw 5,bothwithoutmulti-tasktuning. Thissuggeststhatwhilescalingdatainitiallydrives significantimprovements,beyondacertainthresholdadditionalsyntheticdataoffersdiminishingreturns. 10 Multi-taskTuning: Multi-tasktuningdemonstratedanotableimprovementinaligningmodeloutputswithhuman relevancejudgments. AsshowninTable 4, theimpactofmulti-tasktuningbecomesclearwhencomparingrow 2 (withmulti-task)againstrow 4(without). Underthesame 1.4 Ksynthetictrainingdataset,themodelwithmulti-task tuningachievedNDCGof 0.953 andaccuracyof 63.81,comparedto 0.946 and 62.41 withoutmulti-task. Thisincrease suggeststhatmulti-tasktuning,inconjunctionwithinstructtuning,effectivelyenhancesthemodel\u2019sabilitytogeneralize tohumanrelevancepreferences. Asimilarpatternholdswhencomparingrow 3(withmulti-task)androw 5(without) underthe 2.4 Ksyntheticdataset,whereNDCGimprovesfrom 0.945 to 0.954 andaccuracyfrom 62.70 to 63.55. Tofurthervalidatetheroleofoursyntheticqueries,wefine-tunedthemodelwithoutincludingthesyntheticenterprise querydataset,whilekeepingallotheropen-sourcedatasetsunchanged(i.e.,excludingthedataintheblackboxin Figure 3). Asshowninrow 7. ofTable 4,thisvariantyieldsonlymarginalchangeoverthevanillamodel,withNDCG movingfrom 0.815 to 0.826 andAccuracyfrom 42.16 to 42.88. Thisoutcomealignswithourexpectation: existing open-sourcedatasetsareprimarilydesignedforwebsearchqueryunderstandingandthusdonotsolelycapturethe characteristics of enterprise search relevance labeling. These results confirm that our synthetic enterprise queries providetheprimarytrainingsignal,whilemulti-tasktuningoffersanauxiliarybenefitbyimprovingrobustnessvia multi-tasktuning."
      },
      {
        "paper_id": "b470be6b294b53fc75b90b830d65b5a86ab805a30db5dac3f438f56a929c3232",
        "chunk_id": "b470be6b294b53fc75b90b830d65b5a86ab805a30db5dac3f438f56a929c3232::sec::abstract::chunk::25",
        "section": "abstract",
        "order": 25,
        "text": "Towards open 0040040042304237. The visualization uses dis- reasoningmodelsforopen-endedsolutions. CoRR. tinctcolor-codedregions: thesearchbuffer(green) Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, holds the recently processed data available for Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, matching;thelook-aheadbuffer(yellow)contains ChristosKozyrakis,IonStoica,JosephE.Gonzalez, Clark W. Barrett, and Ying Sheng. 2024. SGLang: thependingsequencetobeencoded; unprocessed Efficient execution of structured language model input is shown in blue; and data that has shifted programs. In Proceedings of the 3.8 th Annual Con- outofthesearchwindowismarkedinred. ference on Neural Information Processing Systems, The algorithm iterates the same core loop. Be- pages 62557\u201362583. lowisatraceofitsexecution: Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. 1. The first eight characters (00400400) are 2024. DistServe: Disaggregating prefill and de- loaded into the look-ahead buffer, while the codingforgoodput-optimizedlargelanguagemodel search buffer is filled with zeros. The algo- serving. InProceedingsofthe 1.8 thUSENIXSympo- rithm finds the longest match for the look- siumonOperatingSystemsDesignandImplementa- tion,pages 193\u2013210. aheadbuffer\u2019sprefixwithinthesearchbuffer. 11 p = 0 l = 2 c = 4 0 0 0 0 0 0 0 0 0 0 4 0 0 4 0 0 4 2 3 0 4 2 3 7 p = 5 l = 6 c = 2 0 0 0 0 0 0 0 4 0 0 4 0"
      },
      {
        "paper_id": "3d18885aa276bd0a730b7111090baba96e61102495bd9db9a783dcc169312975",
        "chunk_id": "3d18885aa276bd0a730b7111090baba96e61102495bd9db9a783dcc169312975::sec::references::chunk::3",
        "section": "references",
        "order": 3,
        "text": "multiple-choice questions, with con- texts ranging from 8. k to 2. M words. It contains the following categories: single-document QA, multi-documentQA,longin-contextlearning,long- dialogue history understanding, code repo under- standing,andlongstructureddataunderstanding. B Accuracy-ResponseLengthTrade-off Table 4 quantifies the accuracy\u2013length trade-off. Tight hard-length constraints reduce average re- sponse length from 8.3 k tokens (Base Model) to 12 GPQA Common- LiveCode- Long- Model MATH-500 AIME 24 AIME 25 Average Diamond senseQA Bench Benchv 2 Acc.\u2191 #Tok.\u2193 Acc.\u2191 #Tok.\u2193 Acc.\u2191 #Tok.\u2193 Acc.\u2191 #Tok.\u2193 Acc.\u2191 #Tok.\u2193 Acc.\u2191 #Tok.\u2193 Acc.\u2191 #Tok.\u2193 Acc.\u2191 #Tok.\u2193 BaseModel 93.6 4,837 75.5 14,191 68.5 17,402 55.5 7,284 83.5 1,130 73.1 10,261 39.7 2,980 69.9 8,298 SFT 94.0 4,292 75.9 12,381 66.1 16,058 55.8 6,459 84.0 939 72.5 9,708 39.5 2,915 69.7 7,536 Hard-Length 1.6 k 94.0 4,388 76.2 12,803 67.0 15,707 55.9 6,712 83.5 1,087 72.5 9,909 39.3 2,822 69.8 7,633 Hard-Length 8. k 93.8 3,369 72.9 10,633 61.9 12,758 54.9 5,535 83.8 959 73.1 8,772 39.9 2,332 68.6 6,337 Hard-Length 8. k\u21924. k 93.4 2,703 71.2 9,114 55.5 10,770 54.5 4,642 83.7 844 71.3 7,972 38.6 1,994 66.9 5,434 Soft-Length 93.4 3,129 72.0 10,105 58.9 11,944 54.9 5,128 83.6 900 71.6 8,335 38.8 2,137 67.6 5,954 Normalized-Length 92.2 1,734 67.0 11,723 50.2 7,475 53.8 3,011 83.7 537 69.4 6,273 37.1 1,326 64.8 4,583 TWYN 94.2 3,377"
      },
      {
        "paper_id": "2558d034ca1cd9d7f81165e7878b8f394ad83d7054c62940ed7e734a86d157bf",
        "chunk_id": "2558d034ca1cd9d7f81165e7878b8f394ad83d7054c62940ed7e734a86d157bf::sec::abstract::chunk::1",
        "section": "abstract",
        "order": 1,
        "text": "Enterprise Web Description Queriesleveragecontextandinternalenti- Queries rely on common knowledge and tiesalongwithsemanticunderstanding generalsemantics Example irisprojectdate structureoftheiris KeyDifference Furtherrequireenterprise-specificknowl- Useglobal/commonknowledge(e.g.,\u201ciris\u201d edge(e.g.,\u201ciris\u201d=colleague/project) =partoftheeye) Table 1: Enterprisevs. websearch: bothcanbesemantic,butenterprisequeriesdependondomain-specificknowledge. Atthesametime,fine-tuningtechniquessuchasLoRA[31]havemadeitfeasibletoadaptLLMstodomain-specific rankingtasks[32]. Moreefficientdistillation-basedmethodshavealsoemerged: [10]demonstratedthatfine-tuned small LLMs could improve the quality of augmented relevance datasets, highlighting the promise of post-training SLMsforscalableevaluation. RRADistill[7]distilledlargemodelsintoSLMsforre-rankinglong-tailqueries,while Rank 1[33]leveragedreasoning-basedLLMs(e.g.,OpenAI\u2019so 1[34],DeepSeek\u2019sR 1[35])andMSMARCO-derived tracestotrainsmaller,explainable,high-performingre-rankers. Collectively,theseapproacheshighlightthepromiseof syntheticdatapipelinesanddistillationforbuildingcompactyeteffectiveretrievalmodels. Despite this progress, the exploration of SLMs for relevance labeling has been largely limited to open-domain, semanticallydrivenretrievaltasks[10]. Whiletheseworksshowedthatcompactmodelscanenhancedatasetquality orapproximatesemanticrelevance,enterprisesearchposesdistinctchallenges. Queriesinenterpriseenvironments oftencombinesparsekeywordmatcheswithsemanticintent,anddocumentcollectionsspanheterogeneousdomains suchasTeamschats,emails,andfiles. Currentindustrialpracticecontinuestorelyonfrontier-scaleLLMs,combined withcarefulpromptengineeringandchain-of-thoughtreasoning[36],tohandlethiscomplexity[8]. Tothebestofour knowledge,nopriorworkhasexaminedwhetherfine-tunedSLMscandelivercompetitiveandresource-efficientlabels inthissetting. OurworkaddressesthisgapbyadaptingPhi-3.5-miniforenterpriserelevancelabeling,demonstrating thatcarefullycuratedtrainingdatasetsanddistillationpipelinescanmakeSLMsafaster,moreefficient,andpractical alternativeforlarge-scaleexperimentation. SyntheticdatagenerationwithLLMstocreatequerieshasproveneffectiveformodeltraininginlow-resourcesettings [37]. Retrievalapproachesfine-tunedsolelyonsyntheticdatahaveoutperformedstrongbaselineslikeBM 2.5 andother self-superviseddenseretrievalmethodsacrossvariousdomain-specificIRtasks [38]. ApproachessuchasPromptagator [39]andInParsv 2 [40]haveenabledthedevelopmentofopen-sourcerankersthatachievestate-of-the-artresultson publicbenchmarks,whileDUQGen[41]exploredunsuperviseddomainadaptationthroughclusteringandprobabilistic samplingtodiversifysyntheticqueries. Buildingontheseinnovations,wefocusonthepracticaldeploymentofaSmallLanguageModel(SLM)labelerfor enterprisesearchandretrieval. Specifically,wefine-tunePhi-3.5 Mini[9]usingdomain-specificdatageneratedthrough anLLM-drivenqueryandlabelpipeline,customizedviainstructiontuningonsyntheticenterprise-stylequeriesand documents. WeselectedPhi-3.5 MiniandGPT-4. oasbenchmarkmodels,giventheirprovenperformanceinrecent retrievalandreasoningstudies[24],andbecausetheyrepresentstrongexemplarsoftheirrespectivemodelclasses (compactSLMsvs. frontier-scaleLLMs). WhilewedidnotincludetheresultswithnewermodelssuchasGPT-5. or alternativeSLMfamilies,webelieveourfindingstogeneralize,sincethecoreadvantagecomesfromthedatageneration anddistillationpipelineratherthananymodel-specificproperty. OurresultingSLMlabelerachievesperformance comparabletofrontierLLMswhiledeliveringsubstantialimprovementsinefficiencyandscalability,makingitwell- suitedforlarge-scaleenterpriserelevanceevaluation. Notably, traditionalapproachestosearchrelevancelabeling, suchasmanualannotationordirectrelianceonfrontierLLMs,presentsignificantchallenges. Manualannotationis ofteninfeasibleinreal-worldenterprisesearchduetostrictprivacypoliciesandthesubstantialeffortrequiredtogo throughlarge-scaledatasets. LLMs-basedlabeling,whilecapableofproducinghigh-qualitylabels,iscomputationally expensiveandslow,makingitunsuitableforlarge-scaledeployment. 2.1 ProblemDefinition Givenauserqueryq,theobjectiveistodeterminetherelevanceofasetofdocumentsD ={d ,d ,...,d }tothequery 1 2 n q.Thiscanbeformalizedaslearningarelevancefunctionr(q,d )thatassignsarelevancescoretoeachquery-document i pair,(q,d ),whered \u2208D. Wedefinetheproblemasfollows: i i Input: \u2022 Aqueryq \u2208Q,whereQisthespaceconsistingofalluserenterprisequeries. \u2022 AsetofdocumentsD ={d ,d ,...,d },whered \u2208DandDarethespaceofdocuments. 1 2 n i 3 Dataset #Samples Description Usage Eyes-onenterprisedocu- 1,500 Aproprietarycollectionofinternaldocumentswithassoci- SupervisedFine-tuning: Usedtogeneratesyn- ments atedmetadata,curatedunderaneyes-onreviewsetting. theticquery\u2013doc\u2013labeltripletsandfine-tunethe modelonthem. INTERS[42] \u223c250,000 samples Multi-tasktuningdatasetcoveringmultipletasks,including SupervisedFine-tuning:Usedformulti-tasktun- on 2.0 differenttasks query,document,andquery-documentrelationshipunder- ing.(GreenboxinFigure 3) standing. TREC-CAsT[6] 4,000 PublicdatasetforConversationalInformationSeeking(CIS) SupervisedFine-tuning:Used\u223c4. ksamplesfor withpassage-query-humanlabeltriplets. instructtuning.(BlueboxinFigure 3) MS MARCO Passage Over 400,000 pas- Opendatasetforinformationretrieval,containingpassage- SupervisedFine-tuning:UsedGPT-4. otogenerate [5] sages querypairsandsomeofthemhavehumanannotationsof syntheticqueriesforinstructtuning.(Purpleboxin scale 0-3. Figure 3) Human-labeled enter- 923 Aproprietarydatasetofenterprisequery\u2013documentpairs, Evaluation:Usedasthegold-standardevaluation prisesearchdataset annotatedbytrainedhumanlabelersforrelevance. datasetforfinalcomparison. Table 2: Datasetsusedforfine-tuningandevaluation Output:"
      },
      {
        "paper_id": "0e1cce3a1d1b5e90d03be157e35c0c92b87c1c484c6149b02b2c6f8d9c4a28c7",
        "chunk_id": "0e1cce3a1d1b5e90d03be157e35c0c92b87c1c484c6149b02b2c6f8d9c4a28c7::sec::results::chunk::0",
        "section": "results",
        "order": 0,
        "text": "stronger backbones and maintaining substantial coverage even with a 2.0 B model. This behavior is consistent with the proposed file-centric state abstraction, which enables the agent to iterate through long document lists without relying on an ever-growing prompt history. In contrast, baseline agents exhibit substantially lower average coverage and high variance across runs. Manual inspection suggests that failures often manifest as early termination, skipping items, or producing summaries that primarily restate titles rather than paper content. Ablation analysis. Removing the file-centric state and relying on compressed long-context prompts substantially degrades coverage across all models (bottom block of Table 1). Even with stronger backbones, the ablated variant shows reduced average coverage and large run-to-run variance. This supports the claim that explicit persistent state externalizationisakeycontributortolong-horizonstabilityandcannotbefullyreplacedbylong-contextcompression alone. Limitations of the comparison. We do not claim absolute superiority over proprietary systems. Commercial agents may employ undocumented optimizations or termination heuristics that trade off completeness for latency or cost. Our goal is to highlight a qualitative difference in long-horizon behavior: explicit externalization of persistent state is associated with higher and more stable coverage under extended execution horizons. 5.3 Case Study: Real-World Applications and Expert Blind Review To validate the framework\u2019s capability, we implemented InfiHelper, a semi-general"
      }
    ],
    "citations": [],
    "metrics": {
      "retrieval_latency": 0.11011171340942383,
      "llm_latency": 0.0,
      "retrieved_chunks": 10,
      "refused": true,
      "refusal_reason": "Max Retries Failed",
      "total_sentences": 0,
      "unaligned_sentences": 0
    }
  },
  "q02::strict": {
    "query": "How does Pooling by Multihead Attention (PMA) differ from EOS and mean pooling for code embeddings?",
    "answer": "Unlike mean pooling and EOS representation, the cross-attention mechanism in PMA allows the model to learn which tokens are most salient for the final representation [1]. PMA also maintains the foundational causal architecture and efficiency of the LLM backbone, as its overhead is negligible compared to the billions of parameters in the LLM [1]. Furthermore, by decoupling the LLM's hidden dimension from the final LLM embedding dimension, PMA can produce compact embeddings suitable for vector databases without requiring computationally expensive MRL training [1].",
    "citations": [
      "22aa53245f6ef08b6d5380a270e0461bcf35fe98232746c006ed9bc7edd3f7de:abstract:22aa53245f6ef08b6d5380a270e0461bcf35fe98232746c006ed9bc7edd3f7de::sec::abstract::chunk::1"
    ],
    "metrics": {
      "retrieval_latency": 0.08236360549926758,
      "llm_latency": 8.082742929458618,
      "retrieved_chunks": 10,
      "refused": false,
      "attempts": 1,
      "safety_check": "passed",
      "total_sentences": 3,
      "unaligned_sentences": 0,
      "confidence_score": 1.5,
      "alignment_score": 1.0,
      "recall_score": 2.0
    }
  },
  "q03::strict": {
    "query": "What backbone models are used to build the C2LLM family?",
    "answer": "The C2LLM family is built upon Qwen-2.5-Coder backbones [10].",
    "citations": [
      "22aa53245f6ef08b6d5380a270e0461bcf35fe98232746c006ed9bc7edd3f7de:abstract:22aa53245f6ef08b6d5380a270e0461bcf35fe98232746c006ed9bc7edd3f7de::sec::abstract::chunk::0"
    ],
    "metrics": {
      "retrieval_latency": 0.09941482543945312,
      "llm_latency": 8.342434644699097,
      "retrieved_chunks": 10,
      "refused": false,
      "attempts": 1,
      "safety_check": "passed",
      "total_sentences": 1,
      "unaligned_sentences": 0,
      "confidence_score": 1.5,
      "alignment_score": 1.0,
      "recall_score": 2.0
    }
  },
  "q04::strict": {
    "query": "How is embedding dimensionality adaptation achieved in C2LLM?",
    "answer": "In C2LLM, the Pooling by Multihead Attention (PMA) module facilitates embedding dimensionality adaptation [7]. This module can reduce the embedding dimension while also pooling information from the token sequence into a single embedding vector [7]. This provides an alternative to MRL for dimensionality adaptation [7].",
    "citations": [
      "22aa53245f6ef08b6d5380a270e0461bcf35fe98232746c006ed9bc7edd3f7de:abstract:22aa53245f6ef08b6d5380a270e0461bcf35fe98232746c006ed9bc7edd3f7de::sec::abstract::chunk::0"
    ],
    "metrics": {
      "retrieval_latency": 0.08883237838745117,
      "llm_latency": 8.616382360458374,
      "retrieved_chunks": 10,
      "refused": false,
      "attempts": 1,
      "safety_check": "passed",
      "total_sentences": 3,
      "unaligned_sentences": 0,
      "confidence_score": 2.5,
      "alignment_score": 1.0,
      "recall_score": 4.0
    }
  },
  "q05::strict": {
    "query": "What training data scale is used for C2LLM and why is it sufficient?",
    "answer": "I cannot answer this reliably with the available evidence (Verification Failed).",
    "evidence": [
      {
        "paper_id": "a398c05fde24df3d45a14b3d37d19a600f23853f459ba220182d5f7ae065f8ea",
        "chunk_id": "a398c05fde24df3d45a14b3d37d19a600f23853f459ba220182d5f7ae065f8ea::sec::results::chunk::0",
        "section": "results",
        "order": 0,
        "text": "tions). Pooled SLMs (65,600 predictions): 96.8% validity, 61.1% accuracy. Best open models (20\u20133.2 B)achieved\u223c99%validityandupto 78%accuracy. Guidedpromptingimprovedvalidity and accuracy relative to zero-shot."
      },
      {
        "paper_id": "de9a3d462343edd0d0fc71b777496825b180bae9c41811e68a2c0eacb17f3c4f",
        "chunk_id": "de9a3d462343edd0d0fc71b777496825b180bae9c41811e68a2c0eacb17f3c4f::sec::abstract::chunk::11",
        "section": "abstract",
        "order": 11,
        "text": "directly at each stage. Training (>2.4 m)toaccommodatelongerobserva- utilizesChain-of-Thoughtrationalesdistilled tionperiods. fromateachermodel. \u2022 GRPO on single stage: GRPO applied di- 4.1.3 DataSplittingandLeakagePrevention rectlytothepredictiontaskateachstage. Topreventdataleakage,weimplementedastrict Patient-Levelsplittingprotocolgovernedbythree \u2022 SFT\u2192SFT:Amulti-stageSFTpipelinecon- principles: sistingofpre-trainingonclinicalindicesfol- lowedbyfine-tuningondiagnosis,servingas 1. Patient-LevelIsolation: DataissplitbyPa- asupervisedcounterparttoourmethod. tient ID to strictly prevent overlap between trainingandtestsets. \u2022 SFT \u2192 GRPO: The conventional RLHF 2. Holistic Test Set Exclusion: Patients re- pipelineconsistingofSFTwarm-uponclini- served for the Stage 2 test set are excluded calindicesfollowedbyGRPOfine-tuning. 5 Table 1: ExperimentalResultsonAsanMedicalCenterDataset. WecompareDementia-R 1. againstgeneral- purposeLLMsandmedical-specificmodels. Boldandunderlineindicatethebestandsecond-bestperformance. All"
      },
      {
        "paper_id": "de9a3d462343edd0d0fc71b777496825b180bae9c41811e68a2c0eacb17f3c4f",
        "chunk_id": "de9a3d462343edd0d0fc71b777496825b180bae9c41811e68a2c0eacb17f3c4f::sec::methods::chunk::1",
        "section": "methods",
        "order": 1,
        "text": "countsareusedforStage 1. training/evaluation. Table 5: TableA 2: TaskdistributionforStage 1. pre- training(AMC). Task Input Kept Train Test Task Train Test MMSE 1,899 1,671 1,331 340 CDRSB 1,882 1,656 1,322 334 MMSE 17,131 4,593 ADAS 11 1,891 1,663 1,311 352 GDS 15,787 3,972 ADAS 13 1,865 1,637 1,307 330 CDR 4,194 1,069 ADASQ 4 1,897 1,669 1,340 329 Total 37,112 9,634 RAVLT_learning 1,885 1,657 1,347 310 Total 11,319 9,953 7,958 1,995 A Appendix filtering and keep 9,953 samples after excluding A.1 DataPreprocessingDetails fine-tuning test participants. No samples are re- A.2 PretrainingDataStatistics(AMC) movedbythetoken-lengthconstraint(\u22648,0.00 to- To prevent patient-level leakage, we exclude all kens) or parsing/ID issues in our pipeline. We patientsreservedfordownstreamfine-tuningfrom bucket the time gap to the prediction target into theStage 1. pretrainingcorpus. Afterremoving 577 1-monthbinsupto 6. monthsandanadditional>6. m fine-tuning test patients from the original cohort bin(seeSec.A.4. forthebucketdefinition). Finally, of 11,1.63 patients,weobtain 46,7.46 longitudinal we perform a patient-level split to create Stage 1 samplesforintermediateclinicalscoreforecasting train/testsets,resultingin 7,9.58 trainingsamples (MMSE/GDS/CDR). We perform a patient-level and 1,995 test samples. Table 6 summarizes the split with a test ratio of 0.20, resulting in 3,568 overall dataset composition, and Table 7 reports trainingpatients(37,1.12 samples)and 8.92 testpa- task-wisestatistics. tients (9,634 samples). For evaluation efficiency, Toadaptdistinctdatamodalitiesforourunified the test split is task-stratified and reduced to 800 reasoning framework, we developed specialized samples (401 patients). Finally, samples exceed- preprocessingpipelinesforbothunstructuredclin- ing 8,000 tokens under the Qwen 2.5-7. B-Instruct ical notes (Asan) and"
      },
      {
        "paper_id": "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be",
        "chunk_id": "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::references::chunk::7",
        "section": "references",
        "order": 7,
        "text": "0.2392 Robust (HC 1) standard errors in parentheses. * p\u00a10.1, ** p\u00a10.05, *** p\u00a10.01 35 Table 12: Scaling Laws for Time Taken over Log Compute (AI Users Only, Grade > 0) (1) (2) Log Time Log Time Log training compute -0.0463 -0.0608 (0.0635) (0.0593) Constant 8.1141*** 8.4926*** (1.5924) (1.5029) Profession \u00d7 Task FE No Yes Observations 316 316 R-squared 0.0017 0.1912 Robust (HC 1) standard errors in parentheses. * p\u00a10.1, ** p\u00a10.05, *** p\u00a10.01 Table 13: Scaling Laws for Time Taken over Log Compute, by Profession (AI Users Only, Grade > 0) (1) (2) (3) Consultants Data Analysts Managers Log training compute 0.0564 -0.0959 -0.0963 (0.1301) (0.0909) (0.0964) Constant 5.5589* 9.3227*** 9.8636*** (3.2798) (2.2841) (2.4264) Task FE Yes Yes Yes Observations 85 131 100 R-squared 0.0910 0.0385 0.3751 Robust (HC 1) standard errors in parentheses. * p\u00a10.1, ** p\u00a10.05, *** p\u00a10.01 36 Figure A 3: Scaling Laws for Earnings Per Minute over Log Compute 2.6 \u2248 $7.20/hour per 1.0 x compute 2.4 2.2 2 1.8 1.6 1.4 1.2 1 0.8 0.6 24 25 26 27 Log training compute )nim/$( etunim rep sgninraE Mean EPM for Human-Only Participants Table 14: Scaling Laws for Earnings Per Minute over Log Compute (AI Users Only, Grade > 0) (1)"
      },
      {
        "paper_id": "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be",
        "chunk_id": "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::references::chunk::8",
        "section": "references",
        "order": 8,
        "text": "(2) EPM ($/min) EPM ($/min) Log training compute 0.1152 0.1213 (0.1238) (0.1181) Constant -1.4430 -1.7521 (3.1080) (3.0359) Profession \u00d7 Task FE No Yes Observations 316 316 R-squared 0.0025 0.1386 Robust (HC 1) standard errors in parentheses. * p\u00a10.1, ** p\u00a10.05, *** p\u00a10.01 37 Figure A 4: Scaling Laws for Total Earnings Per Minute over Log Compute \u2248 $13.20/hour per 1.0 x compute 4 3.5 3 2.5 2 1.5 1 24 25 26 27 Log training compute )nim/$( etunim rep sgninrae latoT Mean TEPM for Human-Only Participants Table 15: Scaling Laws for Total Earnings Per Minute over Log Compute (AI Users Only, Grade > 0) (1) (2) TEPM ($/min) TEPM ($/min) Log training compute 0.2288 0.2211 (0.2168) (0.2127) Constant -3.5457 -3.8920 (5.4446) (5.3639) Profession \u00d7 Task FE No Yes Observations 316 316 R-squared 0.0031 0.1245 Robust (HC 1) standard errors in parentheses. * p\u00a10.1, ** p\u00a10.05, *** p\u00a10.01 38 Table 16: Scaling Laws for Time Taken over Calendar Time (Non-agentic Tasks) (1) (2) Log Time Log Time Months since Nov 2022 -0.0095** -0.0092** (0.0041) (0.0045) Constant 7.0781*** 3.5840*** (0.1156) (0.3345) Profession \u00d7 Task FE Yes Yes Full controls No Yes Observations 266 264 R-squared 0.1255 0.3444 Robust (HC 1) standard errors in parentheses. * p\u00a10.1, **"
      },
      {
        "paper_id": "71b20c8ba98b09d8deee474188fc033cff6d5c02131fa65b222da95127c94283",
        "chunk_id": "71b20c8ba98b09d8deee474188fc033cff6d5c02131fa65b222da95127c94283::sec::abstract::chunk::4",
        "section": "abstract",
        "order": 4,
        "text": "capacity. Thedefaultsetting\u201c8. choose 4\u201d(highlighted)achievesthebestoverallbalance. Accuracy(\u2191) Configuration Dim MMAU MMSU OBQA 8. choose 4(Ours) \ud835\udc6b 61.50 38.19 53.85 1.6 choose 4 \ud835\udc37 60.80 34.77 48.35 8. choose 1 \ud835\udc37 59.89 32.95 43.95 4. choose 2 \ud835\udc37 61.10 38.12 52.74 8. choose 4 \ud835\udc37/2 60.60 37.28 54.72 8. choose 4 2\ud835\udc37 59.69 35.94 50.54 Notably, increasingthetotalnumberofexpertsdoesnotnecessarilyimproveperformance: expandingfrom\u201c8 choose 4\u201dto\u201c1.6 choose 4\u201dconsistentlydegradesperformanceacrossallbenchmarks. Regardingroutingsparsity, strongerconstraintsarenotalwaysbeneficial;activatingtoofewexperts,suchas\u201c8. choose 1\u201d,significantlyharms audioreasoningability,whilemoderatesparsity(specifically\u201c4. choose 2\u201dor\u201c8. choose 4\u201d)yieldsstrongerresults. Furthermore,largerexpertcapacityisnotinherentlybetter: increasingexpertdimensionality(\u201c8. choose 4,2\ud835\udc37\u201d) leads to consistent performance drops, whereas reduced capacity can remain competitive on certain reasoning benchmarks. Crucially,underthesametrainingbudget,configurationswithlargereffectivecapacitythroughmore activeexperts(exemplifiedby\u201c4. choose 2\u201dvs.\u201c8. choose 4\u201d)canachievecomparableorimprovedperformance, highlightingtheimportanceofbalancingmodelcapacityandroutingsparsity. Inconclusion,effectiveaudioreasoningdemandsacarefulbalanceofexpertcount,sparsity,andcapacity,rather thansimplyscalinganysingledimensioninisolation. 4.3.2 ImpactofExpertBalanceLoss To isolate the impact of the expert balance loss (L ) on expert utilization and downstream performance, we aux evaluateavarianttrainedwithoutthisauxiliaryterm(w/oEBL).Table 3. summarizestheresultsacrossMMAU, MMSU,andOBQA. Removingthebalancelossyieldsdivergentoutcomesacrossbenchmarks. Onsemanticreasoningtasks,themodel incorporatingEBLconsistentlyoutperformstheablatedcounterpartby 0.82 and 1.54 points,respectively. This validatesthatloadbalancingeffectivelymitigatesexpertcollapse,ensuringdiverseexpertsaresufficientlytrained\u2014a prerequisiteforhandlingcomplexsemanticreasoninggroundedinworldknowledge. 6 MoEAdapter January 5,2026 Table 3: Ablation on Expert Balance Loss. We report accuracy (%) on audio reasoning and understanding benchmarks. Method MMAU MMSU OBQA Ours(w/EBL) 61.50 38.19 53.85 w/oEBL 63.01 37.37 52.31 Conversely,thew/oEBLvariantachieveshigheraccuracyonMMAU.Weattributethisbehaviortotheintrinsic heterogeneityofMMAU,whichspansmultipleaudiomodalities(speech,sound,andmusic)whilebeingdominated byperceptualandlow-levelacousticpatternsinalargeportionofinstances. Intheabsenceofexplicitloadbalancing, therouternaturallyconcentratesupdatesonasubsetof\u201cdominantexperts\u201dthatarewellalignedwiththesefrequently occurringperceptualandlow-levelacousticpatterns,resultinginimprovedperformanceonperception-oriented audiounderstandingtasks. However,thisconcentrationofexpertutilizationreducestheeffectivediversityofexperts participatinginoptimization,whichlimitsthemodel\u2019sabilitytosupportsystematicgeneralizationandhigher-level semanticreasoning. ThiseffectisreflectedinthedegradedperformanceobservedonMMSUandOBQA,where successfulpredictionrequiresgroundingaudioinputsinworldknowledgeandperformingmorecomplexsemantic inference. Insummary,theexpertbalancelossimposesatrade-off: itslightlyconstrainsperception-orientedperformance tosignificantlyenhancesemanticreasoningbyensuringbalancedexpertutilization. Wenextanalyzetherouting dynamicstoelucidatethismechanism. 5 AnalysisofExpertSpecializationandOptimizationDynamics TheMoE-adapterismotivatedbytheheterogeneousnatureofaudiodata,wheredifferentacousticpatternsand reasoningrequirementsbenefitfromspecializedrepresentations. Inthissection,weanalyzetheinternalbehaviors oftheMoE-adaptertoexplainitsperformancegainsoverdenseadapters,aswellasthetrade-offsobservedwhen introducingexpertbalanceregularization. Ouranalysisfocusesontwocomplementaryaspects: (i)howrouting-based expertspecializationemergesacrossheterogeneousaudiomodalities,and(ii)howsuchspecializationreshapesthe optimizationdynamicsbymitigatinggradientconflicts. 5.1 EffectofExpertBalanceonRoutingandSpecialization Speech Sound Music 0 1 2 3 4 5 6 7 Expert ID ytiladoM oiduA 100 92.8 2.2 12.9 96.6 5.2 98.3 27.3 64.6 80 60 79.6 2.1 29.1 97.1 5.7 97.9 33.7 54.8 40 79.9 2.6 25.8 97.0 5.9 98.4"
      },
      {
        "paper_id": "a398c05fde24df3d45a14b3d37d19a600f23853f459ba220182d5f7ae065f8ea",
        "chunk_id": "a398c05fde24df3d45a14b3d37d19a600f23853f459ba220182d5f7ae065f8ea::sec::conclusion::chunk::9",
        "section": "conclusion",
        "order": 9,
        "text": "= family; marker = reasoning mode. 7 3.4 Reasoning mode effects (Thinking vs Non-thinking) Enabling thinking mode in supporting SLMs improved performance. Aggregated results by rea- soning mode are presented in Table 3.2. Thinking-mode open models showed higher validity and effective accuracy than their non-thinking counterparts; GPT-5.2 retained the highest absolute performance. Table 3.2: Performance by reasoning mode (guided benchmark). Mode Validity(%) Effectiveacc.,collapsed(%) Conditionalacc.,collapsed(%) Non-thinking(OSpooled) 95.4 56.5 59.2 Thinking(OSpooled) 98.9 68.4 69.2 GPT-5.2(Thinking) 99.8 81.1 81.3 3.5 Invalid-output analysis: dominant failure modes Invalid outputs were most frequently classified as \u201cOther\u201d, followed by Missing and Out-of-format (see Supplementary Table 6 for full breakdown). Smaller and instruction-misaligned models (e.g., Gemma 3:2.70 M, SmolLM 2 variants) showed elevated missing and out-of-format rates, explaining much of their low effective accuracy despite occasionally reasonable conditional accuracy among their valid outputs. 3.6 Scaling with model size and task difficulty Performance scaled strongly with model size. Table 3.3 shows binned results: validity rose from 82.9% in the \u22641. B bin to \u223c99.2% in the 10\u20132.9 B bin, while effective collapsed accuracy rose from 27.0% to \u223c73.5% across the same bins. GPT-5.2 exceeded the open-model plateau (81.1% effective accuracy). Table 3.3: Performance by model size bin (guided benchmark). Sizebin Validity(%) Effectiveacc.,collapsed(%) Conditionalacc.,collapsed(%) \u22641. B"
      },
      {
        "paper_id": "06a2328681ecddca675125a619a76177d24c96614cf6f5bd65b3b97050e69bd9",
        "chunk_id": "06a2328681ecddca675125a619a76177d24c96614cf6f5bd65b3b97050e69bd9::sec::abstract::chunk::8",
        "section": "abstract",
        "order": 8,
        "text": "Fig. 1. Output of our system for the German CNE task given only few Input: ... examplesasinput. Output: ... ... CNE English datasets: We use 3 datasets for English: Complete the following text. The instruction was to <COMPLETE> CSTINSIGHT [1] which is a database of customer insights, CDO 435 which is a database from the chief data officer of Our task agnostic meta-prompt for German is \u2013 one of our customers, and TELE 1186 which is a database from a telecommunication customer. Ich gab einem Freund eine Anweisung. The CSTINSIGHT contains 25 tables and 519 cryptic Danach erzeugte er die folgenden columns. Since both DSPy and TextGrad need separate train- Eingabe und Ausgabepaare: ingandvalidationdata,weused 1.9 ofthesetablesfortraining Input: ... and 6 of them for validation for those 2 systems. Output: ... Input: ... For our system as well as for InstInduc, APE and APE Output: ... Zeroshot, we used only 10 of these CSTINSIGHT tables as ... examples (since these 3 systems do not need any tuning). Vervollst\u00a8andigen Sie den folgenden 1. https://en.wikipedia.org/wiki/Jaro\u2013Winkler distance Satz. Die Anweisung lautete: <COMPLETE> 2. https://huggingface.co/meta-llama/Llama-3.3-7.0 B-Instruct The CDO 435 and TELE 1186 datasets were used for [2] N. Mihindukulasooriya, N. S. D\u2019Souza, F. Chowdhury and H. Samu- testing. CDO 435"
      },
      {
        "paper_id": "2558d034ca1cd9d7f81165e7878b8f394ad83d7054c62940ed7e734a86d157bf",
        "chunk_id": "2558d034ca1cd9d7f81165e7878b8f394ad83d7054c62940ed7e734a86d157bf::sec::results::chunk::0",
        "section": "results",
        "order": 0,
        "text": "performancecomparabletothestrongGPT-4. olabeler. Qualitative Analysis: After examining examples from the human-annotated evaluation set, we observe a clear improvementinourfine-tunedlabeleroverthevanillaSLM.Inparticular,theoriginalSLMandevenGPT-4. otendto avoidassigningthelowestrelevancescore,evenfordocumentsthatarecompletelyoff-topic,leadingtooverlyoptimistic relevancejudgments. Bycontrast,ourfine-tunedquerylabelerdemonstratesgreatercalibrationandconfidence: itis 11 Figure 4: Performanceresultscomparingthevanillaandfine-tunedSLMmodelsagainstGPT-4. oontwokeyevaluation metricsfortheenterprisesearchtestdataset. SLM-Human SLM-LLM LLM-Human Row Multi-taskTuning DataRevision SyntheticDataSize NDCG Accuracy(%) NDCG Accuracy(%) NDCG Accuracy(%) 1 \u2013 \u2013 \u2013(VanillaSLM) 0.815 42.15 0.820 41.38 2 \u2713 \u2713 1.4 K 0.953 63.81 0.951 66.16 3 \u2713 \u2713 2.4 K 0.954 63.55 0.950 66.10 0.944 62.58 4 \u2717 \u2713 1.4 K 0.946 62.41 0.950 66.72 5 \u2717 \u2713 2.4 K 0.943 62.50 0.948 66.83 6 \u2717 \u2717 1.4 K 0.943 60.97 0.948 66.05 7 \u2713 \u2713 0(OnlyPublicData) 0.826 42.88 0.824 42.19 Table 4: PerformanceresultscomparingthevanillaandtrainedSLMsunderdifferentfine-tuningconfigurationsfor ablationstudies. ThelastthreecolumngroupspresentthepairwisemetricsbasedonlabelsfromSLM/SLM/LLM againstHuman/LLM/Human. abletocorrectlyassignascoreof 0. whenapassageisentirelyirrelevant,therebyaligningmorecloselywithhuman annotations. Theseresultscollectivelydemonstratethatcarefullyfine-tuningSLMswithhigh-qualitysyntheticdatanotonlyachieves alevelofalignmentwithhumanjudgmentscomparabletothatoflargeLLMs,butalsoenablessubstantiallymore scalableandcost-efficientrelevancelabeling. 5 Conclusion Our work presents an efficient and scalable approach to enterprise search relevance labeling by fine-tuning Small LanguageModels(SLMs)withasyntheticdatagenerationpipeline. Thefine-tunedPhi-3.5 MiniInstructachievesGPT- 4. o\u2013levelaccuracywhilesignificantlyimprovingthroughputandreducingcosts,makinglarge-scalequery\u2013document relevancelabelingpractical. ByleveragingGPT-4. oforquerygeneration,BM 2.5 forhardnegativemining,andLLM- basedjudgmentsforlabeling,wecreatehigh-qualitysynthetictrainingdatathatenablesSLMstoperformcompetitively withLLMs. Empiricalresultsshowourfine-tunedmodelachievesSLMHumanNDCGof 0.953 andpairwiseaccuracyof 63.81, outperformingGPT-4. o(NDCG:0.944,Accuracy: 62.58). Thesefindingshighlightthatcompact,well-trainedSLMs canmatchorevenoutperformlargemodelsinaligningwithhumanrelevancejudgments,whileenablingfasterand moresustainableannotationpipelines. Morebroadly,thisworkhighlightsthepotentialofsyntheticdata\u2013driventraining tobridgethegapbetweenhigh-qualityhuman-alignedevaluationandscalablemodeldevelopment,offeringapractical pathtowarddeployinglightweightmodelsfordata-intensiveNLPtasks."
      },
      {
        "paper_id": "a398c05fde24df3d45a14b3d37d19a600f23853f459ba220182d5f7ae065f8ea",
        "chunk_id": "a398c05fde24df3d45a14b3d37d19a600f23853f459ba220182d5f7ae065f8ea::sec::conclusion::chunk::11",
        "section": "conclusion",
        "order": 11,
        "text": "79.1 Zero-shot 1,500 1,450(96.7) 50(3.3) 96.7 69.6 72.0 Figure 5: Best-per-RADS accuracy by prompting (fig 5). 11 Figure 6: Per-RADS accuracy for selected models under zero-shot versus guided prompting (fig 6). 3.8 Summary of key numeric contrasts Table 3.1(above)summarizesthemaingroup-levelcontrasts: GPT-5.2 exceededpooledopen-source models by +3.0 percentage points in validity and +20.0 percentage points in effective collapsed accuracy (absolute percentage-point differences). These contrasts remained significant in patient- level adjusted models with cluster-robust SEs (all contrasts p < 0.001, FDR-adjusted). 4 Discussion We introduced RXL-RADSet, a radiologist-verified synthetic benchmark of 1,600 reports spanning 10 RADS frameworks, and evaluated 41 open-weight SLM configurations (0.135\u20133.2 B) alongside a proprietary reference (GPT-5.2). Three principal patterns emerged under guided prompting. First, there is a strong size\u2013performance relationship with a marked inflection between sub-1. B and \u22651.0 B models, mirroring broader SLM scaling observations [6, 19]. Second, mid-to-large open models attained near-ceiling validity but persistent classification gaps remained for high-complexity RADS,indicatingthatfailuresareoftenclinicalreasoningerrorsratherthanformatnoncompliance. Third, enabling reasoning scaffolds (Thinking) improved both validity and accuracy, showing that 12 structured reasoning prompts provide benefits beyond enforcing output schema. GPT-5.2 obtained the highest effective collapsed accuracy (81.1%) with near-perfect validity (99.8%), whereas the strongest open-weight models (20\u20133.2 B) reached mid\u2013high 70% effective ac- curacy with \u223c99% validity. These results indicate that carefully"
      }
    ],
    "citations": [],
    "metrics": {
      "retrieval_latency": 0.0906057357788086,
      "llm_latency": 0.0,
      "retrieved_chunks": 10,
      "refused": true,
      "refusal_reason": "Max Retries Failed",
      "total_sentences": 0,
      "unaligned_sentences": 0
    }
  },
  "q06::strict": {
    "query": "How does C2LLM perform on the MTEB-Code benchmark compared to closed-source models?",
    "answer": "I cannot answer this reliably with the available evidence (Verification Failed).",
    "evidence": [
      {
        "paper_id": "0361b6a8418978380390ffcffa15d812f491c3b7d7629bda2f8c3f09c0dbb59a",
        "chunk_id": "0361b6a8418978380390ffcffa15d812f491c3b7d7629bda2f8c3f09c0dbb59a::sec::methods::chunk::0",
        "section": "methods",
        "order": 0,
        "text": "thejudgemodelisprovidedintheappendix. For kens,andthereforeservesasarigorousstresstest completeness, we also report token-level F 1 and forlong-termmemoryretentionandretrievalunder BLEU-1(Papinenietal.,2002). strictcomputationalconstraints. AssummarizedinTable 2, MAGMAachieves 4.2 OverallComparison thehighestaverageaccuracy(61.2%),outperform- Thissectionintroducestheaccuracyperformance ingboththeFull-contextbaseline(55.0%)andthe comparisonbetweenallmethodsontheLoCoMo Nemorisystem(56.2%). Theseresultsindicatethat benchmarkbasedonLLM-as-a-judge. Asshown MAGMAgeneralizeseffectivelytoultra-longinter- in Table 1, MAGMA achieves the highest over- actionhistorieswhilemaintainingstrongretrieval alljudgescoreof 0.7,substantiallyoutperforming precision. theotherbaselines: FullContext(0.481),A-MEM At the same time, the results highlight a favor- (0.58),MemoryOS(0.553)andNemori(0.59)by ableefficiency\u2013granularitytrade-off. Althoughthe relative margins of 18.6% to 45.5%. This re- Full-contextbaselineperformsstronglyonsingle- sult demonstrates that explicitly modeling multi- session-assistanttasks(89.3%),thisperformance relational structure enables more accurate long- comesataprohibitivecomputationalcost,requir- horizonreasoningthanflatorpurelysemanticmem- ingover 1.00 ktokensperquery. MAGMAachieves oryarchitectures. competitive accuracy (83.9%) while using only A closer analysis reveals that MAGMA\u2019s ad- 0.7 k\u20134.2 ktokensperquery,representingareduc- vantage is particularly pronounced in reasoning- tion of more than 95%. This demonstrates that intensive settings. In the Temporal category, MAGMAeffectivelycompresseslonginteraction MAGMAslightlybutconsistentlyoutperformsoth- historiesintocompact,reasoning-densesubgraphs, ers(Judge: 0.650 forMAGMAvs.0.422-0.649 for preservingessentialinformationwhilesubstantially Table 2: Performance comparison on LongMemEval dataset across different question types. We compare our MAGMAmethodagainsttheFull-contextbaselineandtheNemorisystem. QuestionType Full-context Nemori MAGMA (1.01 Ktokens) (3.7\u20134.8 Ktokens) (0.7\u20134.2 Ktokens) inim-o 4-tpg single-session-preference 6.7% 62.7% 73.3% single-session-assistant 89.3% 73.2% 83.9% temporal-reasoning 42.1% 43.0% 45.1% multi-session 38.3% 51.4% 50.4% knowledge-update 78.2% 52.6% 66.7% single-session-user 78.6% 77.7% 72.9% Average 55.0% 56.2% 61.2% Table 3: Systemefficiencycomparisonwithtotalmem- 4.5 AblationStudy orybuildtime(inhours),averagetokenconsumption perquery(inktokens),andaveragequerylatency(in In this subsection, we conduct a systematic abla- seconds). tionstudytoassessthecontributionofindividual componentsinMAGMA.Byselectivelydisabling Method BuildTime(h) Tokens/Query(k) Latency(s) edge types and traversal mechanisms, we isolate FullContext N/A 8.53"
      },
      {
        "paper_id": "a398c05fde24df3d45a14b3d37d19a600f23853f459ba220182d5f7ae065f8ea",
        "chunk_id": "a398c05fde24df3d45a14b3d37d19a600f23853f459ba220182d5f7ae065f8ea::sec::conclusion::chunk::24",
        "section": "conclusion",
        "order": 24,
        "text": "2 0 0 8 Qwen 3:8. b* 3,200 25 4 1 0 20 Qwen 3:1.4 b 1,600 13 2 0 0 11 Qwen 3:3.0 b 1,600 2 0 0 0 2 Qwen 3:3.2 b* 3,200 30 3 2 0 25 Granite 4:3.50 m 1,600 20 2 1 0 17 Granite 4:1. b 1,600 25 3 1 0 21 Granite 4:3. b 1,600 22 2 0 0 20 Gemma 3:2.70 m 1,600 326 59 16 0 251 Gemma 3:1. b 1,600 55 7 2 0 46 Gemma 3:4. b 1,600 10 2 0 0 8 Gemma 3:1.2 b 1,600 12 2 0 0 10 Gemma 3:2.7 b 1,600 15 3 0 0 12 SmolLM 2:3.60 m 1,600 85 13 5 0 67 SmolLM 2:1.7 b 1,600 23 3 1 0 19 Table .7: Performance by RADS complexity bin (guided benchmark) (Supplementary Table 7). Complexitybin Group N(predictions) Valid,n(%) Validity%(95%CI) Effectiveacc.,collapsed%(95%CI) Conditionalacc.,collapsed%(95%CI) OSmodels(pooled) 8,200 7,698(93.9) 93.9(92.7\u201394.9) 73.5(70.2\u201376.5) 78.3(75.0\u201381.3) Minimallycomplex(<5) GPT-5.2(reference) 200 199(99.5) 99.5(98.4\u2013100.0) 91.0(86.7\u201394.8) 91.5(87.2\u201395.3) OSmodels(pooled) 45,100 43,739(97.0) 97.0(96.8\u201397.2) 62.1(60.4\u201363.8) 64.0(62.3\u201365.8) Moderatelycomplex(5\u20138) GPT-5.2(reference) 1,100 1,098(99.8) 99.8(99.5\u2013100.0) 76.9(74.4\u201379.3) 77.0(74.6\u201379.5) OSmodels(pooled) 12,300 12,057(98.0) 98.0(97.3\u201398.7) 49.4(47.2\u201351.7) 50.4(48.1\u201352.8) Highlycomplex(>8) GPT-5.2(reference) 300 300(100.0) 100.0(100.0\u2013100.0) 90.0(86.4\u201393.4) 90.0(86.4\u201393.4) Notes: Effectiveaccuracytreatsinvalidoutputsasincorrect(denominator=allpredictions). Conditionalaccuracyis computedamongvalidoutputsonly. \u201cCollapsed\u201d denotesmajor-categoryscoring;\u201cExact\u201d (notshown)requiresexact subcategorymatch. Table .8: Pooled zero-shot vs guided prompting performance (Supplementary Table 8). Promptingmode N(predictions) Valid,n(%) Invalid,n(%) Validity% Accuracy(effective,all)% Guided 1,500 1,488(99.2) 12(0.8) 99.2 78.5 Zero-shot 1,500 1,450(96.7) 50(3.3) 96.7 69.6 Notes: \u201cAccuracy(effective,all)\u201d treatsinvalidoutputsasincorrect(denominator=allpredictions)."
      },
      {
        "paper_id": "41a1ee585b487b0de127bea91d84527b69149e79f7dc2c45adac124ed8b72247",
        "chunk_id": "41a1ee585b487b0de127bea91d84527b69149e79f7dc2c45adac124ed8b72247::sec::abstract::chunk::7",
        "section": "abstract",
        "order": 7,
        "text": "impart of CFF on the CR and F 1 score, the similarity threshold is set to a fixed value, and the numberofchunkstofuseisvaried. TheevaluationisperformedonLlama 3.1-8. BandQwen 2.5-7.2 Bmodelswith 16 tokensperblockontheLongbenchqmsumdatasetBaietal.(2023). Thisdatasetcontainsrelativelylonginputs,which allowscharacterizingtheCRandaccuracyofCFFwhenappliedtoincreasingnumberofchunks. Figure 6(a)depicts theCRandwhenapplyingCFFtothechunks. Aswesee,theCFFusesupto\u223c3.25\u00d7fewerblocks,forwhichtheir computationcanbereused,thusmitigatingcomputationalandnetworkbottlenecks. Figure 6(b)depictstheF 1. scoreof theCFFwhenscalingthenumberofchunkstofuseonthesametask. TheCFFmanagestokeeptheaccuracyinmost cases,andexperiencesonlyanegligibleaccuracyloss. Table 2. describestheF 1. scoreandtheCRbehaviorwhenapplyingCFFtoevery 8. chunksforvariousthresholdsina varietyofLongbenchtasks (Baietal.,2023). Thetablehighlightsconsistentgainsincompressionwithminimalorno lossinaccuracy. Notably,evenatlowerthresholds,theF 1. scoresremaincomparabletothebaseline,whileachieving compressionratiosofupto\u00d71.87. 6.4 End-to-EndServingPerfromance WeevaluateBFFonasingle-machinevLLMservingbenchmarkoverarandomdatasetof 1.00 requestswithinputsize 1.024 andvaryingnumberofoutputtokens(withrandom-ratio 0.5)usingLlama-3.1-8. B.Afterthefusingblocks(with 8 APREPRINT-JANUARY 7,2026 3.0 2.5 2.0 1.5 1.0 0 5 10 15 20 25 30 #chunks oitar noisserpmoc 88 86 baseline CFF CR Llama 3.1-8. B 84 CFF CR Qwen 2.5-7.2 B 82 0 5 10 15 20 25 30 #chunks (a) erocs 1. F baseline Llama 3.1-8. B CFF Llama 3.1-8. B baseline Qwen 2.5-7.2 B CFF Qwen 2.5-7.2 B (b) Figure 6: CR (a) and F 1 score (b) of CFF vs. number of chunks for Llama 3.1-8. B and Qwen 2.5-7.2 B using the Longbenchqmsumdataset. Table 2: F 1. scoreandCR(inparenthesis)achievedbyCFFforLlama 3.1-8. B. Model Method LCC RepoBench-P PR-en TREC 2. wikimqa GovReport MQA-zh Average CFF 75.77 74.07 21.04 40.48 45.9 83.8 27.34 52.63 thr=0.62 (\u00d71.87) (\u00d71.59) (\u00d71.32) (\u00d71.43) (\u00d71.38) (\u00d71.59) (\u00d71.53) (\u00d71.53) CFF 77.12 74.8 18.3 40.3 45.88 81.39 38.08 53.7 Llama 3.1-8. B thr=0.64 (\u00d71.49) (\u00d71.3) (\u00d71.15) (\u00d71.16) (\u00d71.17) (\u00d71.25) (\u00d71.24) (\u00d71.25) CFF 77.24 75.99 18.38 41.15 44.94 83.83 40.31 54.55 thr=0.66 (\u00d71.26) (\u00d71.15) (\u00d71.06) (\u00d71.06) (\u00d71.06) (\u00d71.09) (\u00d71.1) (\u00d71.11) Baseline 77.73 75.57 18.74 40.95 44.12 82.81 39.18 54.16 \u2018 similarityabovethreshold 0.7),themodelrunnersendstheschedulertheupdatedblock-tables,allowingthescheduler tofreeunusedblocks,andincreasereferencecountsforsharedblocksforfurtherreuseacrossrequests. Figure 7. depictsthethroughputandend-o-endperformanceachievedbyournaiveimplementation,wheretheentire fusionoverheadisreflectedinthedecodephase. Remarkably,theBFFyieldsmeasurablesystem-levelbenefits: lower prefilllatency(TTFT),highereffectivebatching,andincreasedthroughput. TheDecode-phaseinter-tokenlatency(ITL) increasedduetolargerbatchGEMMs,andthefusionoverhead. However,theoverallservingthroughputandrequest"
      },
      {
        "paper_id": "71b20c8ba98b09d8deee474188fc033cff6d5c02131fa65b222da95127c94283",
        "chunk_id": "71b20c8ba98b09d8deee474188fc033cff6d5c02131fa65b222da95127c94283::sec::abstract::chunk::4",
        "section": "abstract",
        "order": 4,
        "text": "capacity. Thedefaultsetting\u201c8. choose 4\u201d(highlighted)achievesthebestoverallbalance. Accuracy(\u2191) Configuration Dim MMAU MMSU OBQA 8. choose 4(Ours) \ud835\udc6b 61.50 38.19 53.85 1.6 choose 4 \ud835\udc37 60.80 34.77 48.35 8. choose 1 \ud835\udc37 59.89 32.95 43.95 4. choose 2 \ud835\udc37 61.10 38.12 52.74 8. choose 4 \ud835\udc37/2 60.60 37.28 54.72 8. choose 4 2\ud835\udc37 59.69 35.94 50.54 Notably, increasingthetotalnumberofexpertsdoesnotnecessarilyimproveperformance: expandingfrom\u201c8 choose 4\u201dto\u201c1.6 choose 4\u201dconsistentlydegradesperformanceacrossallbenchmarks. Regardingroutingsparsity, strongerconstraintsarenotalwaysbeneficial;activatingtoofewexperts,suchas\u201c8. choose 1\u201d,significantlyharms audioreasoningability,whilemoderatesparsity(specifically\u201c4. choose 2\u201dor\u201c8. choose 4\u201d)yieldsstrongerresults. Furthermore,largerexpertcapacityisnotinherentlybetter: increasingexpertdimensionality(\u201c8. choose 4,2\ud835\udc37\u201d) leads to consistent performance drops, whereas reduced capacity can remain competitive on certain reasoning benchmarks. Crucially,underthesametrainingbudget,configurationswithlargereffectivecapacitythroughmore activeexperts(exemplifiedby\u201c4. choose 2\u201dvs.\u201c8. choose 4\u201d)canachievecomparableorimprovedperformance, highlightingtheimportanceofbalancingmodelcapacityandroutingsparsity. Inconclusion,effectiveaudioreasoningdemandsacarefulbalanceofexpertcount,sparsity,andcapacity,rather thansimplyscalinganysingledimensioninisolation. 4.3.2 ImpactofExpertBalanceLoss To isolate the impact of the expert balance loss (L ) on expert utilization and downstream performance, we aux evaluateavarianttrainedwithoutthisauxiliaryterm(w/oEBL).Table 3. summarizestheresultsacrossMMAU, MMSU,andOBQA. Removingthebalancelossyieldsdivergentoutcomesacrossbenchmarks. Onsemanticreasoningtasks,themodel incorporatingEBLconsistentlyoutperformstheablatedcounterpartby 0.82 and 1.54 points,respectively. This validatesthatloadbalancingeffectivelymitigatesexpertcollapse,ensuringdiverseexpertsaresufficientlytrained\u2014a prerequisiteforhandlingcomplexsemanticreasoninggroundedinworldknowledge. 6 MoEAdapter January 5,2026 Table 3: Ablation on Expert Balance Loss. We report accuracy (%) on audio reasoning and understanding benchmarks. Method MMAU MMSU OBQA Ours(w/EBL) 61.50 38.19 53.85 w/oEBL 63.01 37.37 52.31 Conversely,thew/oEBLvariantachieveshigheraccuracyonMMAU.Weattributethisbehaviortotheintrinsic heterogeneityofMMAU,whichspansmultipleaudiomodalities(speech,sound,andmusic)whilebeingdominated byperceptualandlow-levelacousticpatternsinalargeportionofinstances. Intheabsenceofexplicitloadbalancing, therouternaturallyconcentratesupdatesonasubsetof\u201cdominantexperts\u201dthatarewellalignedwiththesefrequently occurringperceptualandlow-levelacousticpatterns,resultinginimprovedperformanceonperception-oriented audiounderstandingtasks. However,thisconcentrationofexpertutilizationreducestheeffectivediversityofexperts participatinginoptimization,whichlimitsthemodel\u2019sabilitytosupportsystematicgeneralizationandhigher-level semanticreasoning. ThiseffectisreflectedinthedegradedperformanceobservedonMMSUandOBQA,where successfulpredictionrequiresgroundingaudioinputsinworldknowledgeandperformingmorecomplexsemantic inference. Insummary,theexpertbalancelossimposesatrade-off: itslightlyconstrainsperception-orientedperformance tosignificantlyenhancesemanticreasoningbyensuringbalancedexpertutilization. Wenextanalyzetherouting dynamicstoelucidatethismechanism. 5 AnalysisofExpertSpecializationandOptimizationDynamics TheMoE-adapterismotivatedbytheheterogeneousnatureofaudiodata,wheredifferentacousticpatternsand reasoningrequirementsbenefitfromspecializedrepresentations. Inthissection,weanalyzetheinternalbehaviors oftheMoE-adaptertoexplainitsperformancegainsoverdenseadapters,aswellasthetrade-offsobservedwhen introducingexpertbalanceregularization. Ouranalysisfocusesontwocomplementaryaspects: (i)howrouting-based expertspecializationemergesacrossheterogeneousaudiomodalities,and(ii)howsuchspecializationreshapesthe optimizationdynamicsbymitigatinggradientconflicts. 5.1 EffectofExpertBalanceonRoutingandSpecialization Speech Sound Music 0 1 2 3 4 5 6 7 Expert ID ytiladoM oiduA 100 92.8 2.2 12.9 96.6 5.2 98.3 27.3 64.6 80 60 79.6 2.1 29.1 97.1 5.7 97.9 33.7 54.8 40 79.9 2.6 25.8 97.0 5.9 98.4"
      },
      {
        "paper_id": "6b5f0ce9df4f0a9e84be6ea4797edc6f1526fa59bed285cbfedac42c5d6a6a5a",
        "chunk_id": "6b5f0ce9df4f0a9e84be6ea4797edc6f1526fa59bed285cbfedac42c5d6a6a5a::sec::abstract::chunk::7",
        "section": "abstract",
        "order": 7,
        "text": "45.8 44.4 31.9 26.5 37.4 34.6 Phi-3.5(Abdinetal.,2024) 4. B \u2713 30.7 51.0 22.1 14.7 31.6 21.1 29.2 27.8 23.0 22.2 20.8 23.3 \u2717 35.2 51.0 30.5 26.8 34.6 31.4 45.8 35.2 26.7 23.5 34.3 32.7 Gemma-3(Team,2.025 a) 4. B \u2713 46.2 47.0 32.6 29.0 37.5 31.9 25.0 40.7 32.6 26.9 32.1 38.0 Ours 3. B \u2013 64.3(+15.2%) 62.9(+9.0%) 44.7(+6.9%) 35.9 44.1 49.2 50.0 55.6 48.9 41.0 44.9 47.7 Table 1: Overall accuracy (%) of different MLLMs (with \u2713 and without \u2717 CoT) and our method across three benchmarks: DeepEval,YesBut,andCII-Bench 1(evaluatedbydomainsandemotions). Thebestandsecond-best sufficientlocalizationofvisualevidence,andsatir- tural,andnarrativenatureofcomicsmakesCVQA icaltargetconfusionindicatesweakalignmentbe- astresstestwherefluentreasoningoftenbecomes tweenintermediatereasoningandthefinalpredic- unfaithful. tion. Whilethisanalysisispurelyempirical,these Toaddressthis,weproposeamodularreasoning correspondencesprovideintuitionforwhystructur- framework that enforces structured, interpretable, ingandverifyingintermediatereasoningstepscan andreward-alignedreasoningforcompactmodels, bebeneficialincomic-basedVQA.Amoreformal withoutrelyingonincreasedscale. Thisapproach treatment of this connection is presented in Sec- consistentlyimprovesperformanceacrossmultiple tion 2.2.3. challengingbenchmarksandenablessmallmodels tomatchorevensurpasslargercounterparts. 3.4 AblationStudy Morebroadly,ourfindingssuggestthateffective WeconductablationexperimentsontheDeepEval multimodalreasoningrequiresstructureratherthan dataset to evaluate the impact of each component longerrationalesorlargermodels. Byexposingthe inourframework,including: (a)directlyprompting limitsofstandardCoTinCVQA\u2014arepresentative the MLLM to generate CoTs and answers; (b) us- settingforreal-worldtasksinvolvingculturalcon- ingonlysupervisedfine-tuning(SFT)withMoCoT- textandvisualabstraction\u2014thisworkpointstoward generateddata;(c)applyingGRPO-basedreinforce- morereliablereasoningframeworksunderrealistic mentfine-tuningdirectlyontheMLLMwithaccu- resourceconstraints. Limitations sessed via qualitative analysis and human annota- tion. Designingfullyautomatic,reliablemetricsfor Our framework consists of two stages, and each high-levelsymbolicreasoningincomicsremainsan stageintroducesitsownlimitations,whichwedis- openchallengeandisbeyondthescopeofthiswork. cussbelow. We view these limitations as natural trade-offs of Dependence on Instruction-Following Ability. optimizingreasoningfaithfulnessatthetrajectory Since GRPO optimizes model behavior based on level,andbelievethatfutureadvancesininstruction- structured outputs and reward signals,"
      },
      {
        "paper_id": "1079d1c0afe1fafff5f6eb41a46dd7c95685a2e173fd91dc87b5537947389678",
        "chunk_id": "1079d1c0afe1fafff5f6eb41a46dd7c95685a2e173fd91dc87b5537947389678::sec::abstract::chunk::10",
        "section": "abstract",
        "order": 10,
        "text": "55.62 40.69 55.52 visualizationcanaugmentthetimeseriesreasoning visual+CoT 51.61 63.71 67.00 61.24 42.07 56.55 capabilityofMLLM,withtheF 1. scoreincreasing visual+ICL 62.50 71.88 65.50 62.02 44.14 58.10 by 13.11%onaverage. Moreover,ontwomodali- RationaleTS 69.76 77.32 71.50 72.87 66.21 74.66 ties,bothCoTandICLcanimprovethereasoning performance. OurproposedRationaleTSoutper- formsonalldatasets,indicatingthetheeffective- Baselines. We conduct the comparison experi- nessofin-contextinference,groundedonthehigh- mentswiththreetypesofbaselines. (1)Timese- qualityrationalebaseandhybridretrieval. riesreasoningmethodswithLLMs: Moirai(Woo et al., 2024), ChatTS (Xie et al., 2025), Chat- Table 3:AblationresultsonFinanceandPowerdatasets. Time (Wang et al., 2025), and TimeXL (Jiang Bold: thebest. Underline: thesecondbest. et al., 2025); (2) VL-Time (Liu et al., 2025) with different base MLLMs: GPT-4. o (Hurst Datasets Finance Power etal.,2024),GPT-52,gemini-2.0-flash(Teametal., Variants F 1 AUC F 1 AUC 2023),gemini-2.5-flash(Comanicietal.,2.025 b), A.1 64.11 73.08 62.50 63.38 w/chart qwen-vl-max (Bai et al., 2025), and qwen 3-vl- A.2 64.92 73.69 68.00 67.49 w/label plus(Yangetal.,2025). (3)Weevaluatetheper- A.3 w/both 62.50 71.88 64.50 65.08 formance of a same MLLM (GPT-4. o-mini) in B.1 w/odata 64.11 70.38 52.50 58.82 B.2 66.13 74.60 66.50 64.73 the textual and visual modality. We also aug- w/osemantic B.3 57.08 60.92 63.50 59.21 random menttheMLLMwithCoT(Weietal.,2022)and RationaleTS 69.76 77.32 71.50 72.87 In-Context Learning (ICL) (Wang et al., 2.024 a). ThedetailedpromptsareprovidedinAppendixD. 4.3 Analysis Implementations. WeemployGPT-5. togenerate 4.3.1 AblationStudy rationalesandGPT-4. o-minitogeneratetextsum- TheablationresultsarereportedinTable 3. InA.1- mary and perform the final prediction. We adopt A.3, we integrate the visual charts, ground-truth text-embedding-3-largeasthetextembedding labels,orbothintorationalesforin-contextinfer- model. The datasets are divided with the ratio of"
      },
      {
        "paper_id": "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755",
        "chunk_id": "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::results::chunk::2",
        "section": "results",
        "order": 2,
        "text": "1.6 3.6 29.9 34.1 24.2 18.7 Semi-AR 77.9 27.6 27.7 32.6 0.0 33.2 Fast-dLLM 78.2 28.4 28.6 11.4 0.3 29.4 PC-Sampler 79.3 34.0 28.6 36.3 27.6 41.2 w/ E-BON 0.5\u2191 1.0\u2191 0.2\u2191 5.9\u2191 0.6\u2191 1.6\u2191 w/ E-SMC 1.9\u2191 1.6\u2191 0.3\u2191 4.1\u2191 1.6\u2191 1.9\u2191 LLaDA-1.5-8. B Uniform 52.7 20.0 28.1 15.8 3.4 24.0 Confidence 19.2 5.4 29.0 33.8 24.8 22.4 Entropy 12.1 5.0 28.8 34.7 0.2 16.2 Margin 27.9 6.4 28.6 31.8 33.6 25.7 P 2 25.4 5.6 28.8 33.6 27.8 24.2 EB-Sampler 12.3 4.8 28.6 34.6 1.6 16.4 Semi-AR 80.7 34.2 26.1 32.4 0.0 34.7 Fast-dLLM 80.8 31.2 27.9 32.9 0.4 34.6 PC-Sampler 82.2 37.4 28.8 35.0 33.4 43.4 w/ E-BON 0.7\u2191 0.8\u2191 0.1\u2191 5.2\u2191 0.8\u2191 1.5\u2191 w/ E-SMC 1.0\u2191 1.2\u2191 0.2\u2191 4.3\u2191 0.2\u2191 1.4\u2191 Pathsearchalgorithmsenhancedecodingstrategies. WedemonstratetheefficacyofE-BONand E-SMC throughtwoexperiments. First,toestablishtheirmaximumpotential,weapplythemto 9 PREPRINT PC-Sampler(Huangetal.,2.025 a)onLLaDAmodels. Thiscombinationsetsanewstate-of-the-art, achievingsubstantialaccuracygainsacrossfivechallengingbenchmarksasshowninTable 2. Second, tovalidatetheirbroadapplicability,weintegrateourmethodswithfivedifferentbaselinesamplers on Open-dCoder model. As shown by the average accuracy improvements in Figure 4, both algorithmsconsistentlyboosteverytestedsampler,confirmingtheyactasversatileenhancersfora widearrayofdecodingstrategies. Path-level optimization is effective for complex reasoning and planning. The gains are most pronounced on benchmarks requiring multi-step reasoning and planning. For example, on LLaDA-Instruct-8. B,E-SMCimprovesGSM 8. Kaccuracyfrom 79.3%to 81.2%(+1.9%)and theCountdownplanningfrom 36.3%to 40.4%(+4.1%). Theseresultsdemonstratethatperforming wellonsuchtasksdependsonadoptingaglobalviewoftheentiresolution. Ourpathsearchalgo- rithmsenablethisperspectivebyutilizingDenoisingEntropytoevaluatethefullgenerationpath, ratherthanassessingonlyasinglestep. 4.4 OTHERRESULTSANDABLATIONSTUDIES Path, H , and accuracy. We validate Denoising Entropy as a proxy for task performance on"
      },
      {
        "paper_id": "0361b6a8418978380390ffcffa15d812f491c3b7d7629bda2f8c3f09c0dbb59a",
        "chunk_id": "0361b6a8418978380390ffcffa15d812f491c3b7d7629bda2f8c3f09c0dbb59a::sec::references::chunk::7",
        "section": "references",
        "order": 7,
        "text": "mechanism. The Judge LLM evaluates the etersandstoragesettingstoreflecttheirstan- alignmentbetweenthegeneratedresponseandthe dardout-of-the-boxperformance. groundtruthusingthefollowingschema. \u2022 UnifiedBackboneModel: Toeliminateper- SystemPrompt: SemanticGrader formance variance caused by different foun- Youareanexpertevaluatorassessingthesemantic dationmodels,allsystemsutilizedOpenAI\u2019s fidelityofamemoryretrievalsystem.Scorethe gpt-4. o-miniforbothretrievalreasoningand Candidate AnsweragainsttheGold Referenceon responsegeneration. acontinuousscale[0.0,1.0]. ScoringRubric: \u2022 1.0(ExactAlignment):Capturesallkeyentities, \u2022 UnifiedEvaluation: Allsystemoutputswere temporalmarkers,andcausalrelationships. Semanticallyequivalent. evaluatedusingtheidenticalLLM-as-a-Judge \u2022 0.8(SubstantiallyCorrect):Mainpointis framework (also powered by gpt-4. o-mini accuratebutlacksminornuancesorsecondary with temperature=0.0), as detailed in Ap- details. pendixC. DatasetStatistics. Weconductedacomprehen- entity in the canyon photo from the \"son\" entity sive evaluation on the full LoCoMo benchmark, involvedinthecaraccident,MAGMAsynthesized testingacrossallfivecognitivecategoriestoassess thesedistinctnodes. Itcorrectlydeducedthatthe varyinglevelsofretrievalcomplexity. Thedetailed \"son\"(referencedlateras\"brother\")wasanaddi- distributionofquerytypesispresentedinTable 6. tional individual, summing up to a count of \"at leastthree,\"alogicalleapimpossibleforsystems Table 6:DistributionofquerycategoriesintheLoCoMo relyingsolelyonvectorsimilarity. benchmarkusedforevaluation. Case 3: Temporal Grounding. When asked QueryCategory Count \"When did she hike?\", baselines either halluci- Single-HopRetrieval 841 nated or defaulted to the conversation timestamp Adversarial 446 (Oct 20). This ignores the semantic meaning TemporalReasoning 321 of the user\u2019s statement: \"we just did it yester- Multi-HopReasoning 282 OpenDomain 96 day.\"MAGMA\u2019sstructuredingestionpipelinenor- TotalSamples 1,986 malizes relative dates during graph construction. The event was stored with the resolved attribute date=\"2023-10-19\", making the retrieval trivial E CaseStudy andexact,completelybypassingtheambiguitythat confusedtheLLM-basedbaselines. To demonstrate MAGMA\u2019s reasoning capabili- tiesacrossdifferentcognitivemodalities,weana- F MetricValidationAnalysis lyzethreereal-worldscenariosfromtheLoCoMo benchmark. Table 7. providesaside-by-sidecom- To validate our choice of using an LLM-based parison of MAGMA against key baselines (A- Judge over traditional lexical metrics, we"
      },
      {
        "paper_id": "a398c05fde24df3d45a14b3d37d19a600f23853f459ba220182d5f7ae065f8ea",
        "chunk_id": "a398c05fde24df3d45a14b3d37d19a600f23853f459ba220182d5f7ae065f8ea::sec::results::chunk::0",
        "section": "results",
        "order": 0,
        "text": "tions). Pooled SLMs (65,600 predictions): 96.8% validity, 61.1% accuracy. Best open models (20\u20133.2 B)achieved\u223c99%validityandupto 78%accuracy. Guidedpromptingimprovedvalidity and accuracy relative to zero-shot."
      },
      {
        "paper_id": "0361b6a8418978380390ffcffa15d812f491c3b7d7629bda2f8c3f09c0dbb59a",
        "chunk_id": "0361b6a8418978380390ffcffa15d812f491c3b7d7629bda2f8c3f09c0dbb59a::sec::methods::chunk::1",
        "section": "methods",
        "order": 1,
        "text": "1.74 thesourcesofitsreasoningcapability. Theresults A-MEM 1.01 2.62 2.26 inTable 4. revealthreemainfindings. MemoryOS 0.91 4.76 32.68 Nemori 0.29 3.46 2.59 First, removing Traversal Policy results in the MAGMA 0.39 3.37 1.47 largestperformancedrop,withtheJudgescorede- creasing from 0.700 to 0.637. This confirms that Table 4:Breakdownanalysisontheperformanceimpact intent-awareroutingiscritical: withoutit,retrieval ofdifferentschemesinMAGMA. degeneratesintoastaticgraphwalkthatintroduces structurallyirrelevantinformationanddegradesrea- MAGMAschemes Judge F 1 BLEU-1 soning quality. Second, removing either Causal w/oAdaptivePolicy 0.637 0.413 0.357 LinksortheTemporalBackboneleadstocompara- w/oCausalLinks 0.644 0.439 0.354 w/oTemporalBackbone 0.647 0.438 0.349 bleandsubstantialperformancelosses(0.644 and w/oEntityLinks 0.666 0.451 0.363 0.647, respectively), indicating that causal struc- MAGMA(Full) 0.700 0.467 0.378 ture and temporal ordering provide complemen- tary,non-substitutableaxesofreasoning. Finally, removing Entity Links causes a smaller but con- reducinginference-timeoverhead. sistentdecline(0.700 to 0.666),highlightingtheir roleinmaintainingentitypermanenceandreducing 4.4 SystemEfficiencyAnalysis hallucinationsinentity-centricqueries. ToevaluatethesystemefficiencyofMAGMA,two metrics are focused: (1) memory build time (the 5 Conclusion timerequiredtoconstructthememorygraph)and (2) token cost (the average tokens processed per We introduced MAGMA, a multi-graph agentic query). memory architecture that models semantic, tem- Table 3 reports the comparative results. While poral, causal, and entity relations within a uni- A-MEM achieves the lowest token consumption fied yet disentangled memory substrate. By for- (2.62 k)duetoitsaggressivesummarization,itsac- mulatingretrievalasapolicy-guidedgraphtraver- rifices reasoning depth (see Table 1). In contrast, sal and decoupling memory ingestion from asyn- MAGMAachievesthelowestquerylatency(1.47 s) chronous structural consolidation, MAGMA en- about 40%fasterthanthenextbestretrievalbase- ableseffectivelong-horizonreasoningwhilemain- line (A-MEM) while maintaining a competitive taining low inference-time latency. Empirical re-"
      }
    ],
    "citations": [],
    "metrics": {
      "retrieval_latency": 0.11570501327514648,
      "llm_latency": 0.0,
      "retrieved_chunks": 10,
      "refused": true,
      "refusal_reason": "Max Retries Failed",
      "total_sentences": 0,
      "unaligned_sentences": 0
    }
  },
  "q08::strict": {
    "query": "What are scaling laws for economic productivity in LLM-assisted work?",
    "answer": "Scaling laws for economic productivity in LLM-assisted work describe empirical relationships between the computational scale and algorithmic progress of Large Language Models (LLMs) and human professional productivity [3]. These laws quantify how improvements in LLMs translate into tangible economic productivity [6]. Specifically, they map the computational scale and algorithmic progress of LLMs to human professional productivity [3]. These scaling laws have been derived experimentally, showing that frontier model progress significantly reduces task completion times, increases earnings per minute, and raises total earnings capacity [4]. The progress of frontier AI is decomposed into two components: sheer scale (increases in training compute) and algorithmic progress (improvements in architectures, training procedures, data, and alignment) [7]. Compute scaling drives approximately 56% of the total reduction in time, while algorithmic advancements account for the remaining 44% [9]. These findings suggest that continued model scaling could boost U.S. productivity by approximately 20% over the next decade [5].",
    "citations": [
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::10",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::0",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::0",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::1",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::14",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::3"
    ],
    "metrics": {
      "retrieval_latency": 0.08514213562011719,
      "llm_latency": 8.830681562423706,
      "retrieved_chunks": 10,
      "refused": false,
      "attempts": 1,
      "safety_check": "passed",
      "total_sentences": 7,
      "unaligned_sentences": 0,
      "confidence_score": 5.5,
      "alignment_score": 1.0,
      "recall_score": 10.0
    }
  },
  "q09::strict": {
    "query": "How does increased training compute translate into task completion time reduction?",
    "answer": "Increased training compute translates into task completion time reduction through a scaling law where a tenfold (10x) increase in model training compute is associated with a 6.3% reduction in time taken [173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::2]. Specifically, a 10x increase in training compute corresponds to approximately a 6% reduction in task completion time (p < 0.31) [173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::2]. This \"compute progress\" contributes to productivity gains by reducing the time required to complete tasks [173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::1]. Raw scale, or training compute, contributes a 4.8% reduction in task time annually [173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:",
    "citations": [
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::2",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::1",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::2",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::3",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:references:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::references::chunk::9",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::16",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::11",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::9",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::18",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::15"
    ]
  },
  "q10::strict": {
    "query": "What is the relative contribution of compute scaling versus algorithmic progress?",
    "answer": "The relative contribution of compute scaling versus algorithmic progress to productivity gains is approximately 56% for compute scaling and 44% for algorithmic advancements [173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::3]. Another decomposition suggests that roughly 58% of efficiency improvements are attributable to the scaling of raw compute, while the remaining 42% stem from algorithmic innovations and software optimization [173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::11]. However, there is some uncertainty in the exact split between these factors, as the specific contribution of compute is estimated with less precision (p \u2248 0.31) compared to the aggregate time trend (p < 0.05) [173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::3].",
    "citations": [
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::3",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::15",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::2",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::2",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::1",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::11",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::7",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::3",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::12",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::5"
    ]
  },
  "q11::strict": {
    "query": "How do productivity gains differ between agentic and non-agentic tasks?",
    "answer": "Productivity gains differ between agentic and non-agentic tasks, with non-agentic tasks experiencing significantly larger benefits from AI assistance [173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::12, 173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::11].\n\nSpecifically, for Total Earnings Per Minute (TEPM), AI increases earnings by $1.58 per minute on non-agentic tasks (p < 0.001), compared to only $0.34 per minute on agentic tasks (p = 0.46). The difference between these effects is statistically significant at the 5% level (p = 0.043) [173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::13].\n\nIn terms of Earnings Per Minute (EPM), AI usage results in an increase of $0.83 per minute on non-agentic tasks [173b0e88ded15dc1",
    "citations": [
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::13",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::8",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::9",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::10",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::18",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::0",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::12",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::11",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:references:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::references::chunk::10",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:references:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::references::chunk::5"
    ]
  },
  "q12::strict": {
    "query": "What experimental design was used to measure human-AI productivity?",
    "answer": "The experimental design used to measure human-AI productivity involved comparing three distinct modes of production: humans working alone, humans working with AI assistance, and the raw output of AI models themselves [173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::4]. This design allowed for a clean comparison of the impact of AI assistance on worker productivity and output quality across different professions [173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::9]. The study also focused on non-agentic tasks for analyzing output quality [173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::3]. Participants were restricted to standard chatbot interfaces with text-based input and output and limited tool access, which may underestimate AI capabilities, particularly for agentic tasks [173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173",
    "citations": [
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::9",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::4",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::1",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::13",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::4",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::3",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::5",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::10",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::9",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::7"
    ]
  },
  "q13::strict": {
    "query": "Why does human-assisted output quality stagnate across model generations?",
    "answer": "Human-assisted output quality stagnates across model generations because human users effectively cap the realized capabilities of frontier models, satisfying to a fixed quality threshold rather than maximizing the tool\u2019s potential [173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::abstract::chunk::3]. For weaker models, human intervention is additive, but for the strongest models, human intervention appears to be destructive, actively degrading the output and bringing the final grade down [173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::results::chunk::6]. Humans are only able to improve model outputs that are slightly better than their own [173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::results::chunk::6]. For intermediate models, humans are unable to add to output quality, and for the best models, humans regress the output towards their own quality levels [173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::results::chunk::6]. This leads to a situation where scaling laws for realized quality are broken by the human user, with users reverting to a regression to the mean regardless of the sophistication of the tool [173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::results::chunk::7]. The scaling law for quality completely disappears when a human is in the loop, and the final output quality remains stagnant at approximately 4.3 points [173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a",
    "citations": [
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::6",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::7",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::5",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:references:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::references::chunk::14",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::3",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::3",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::1",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::10",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::1",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::2"
    ]
  },
  "q15::strict": {
    "query": "What are the three types of noise identified in LLM evaluations?",
    "answer": "The three types of noise identified in LLM evaluations are: prediction noise, data noise, and total noise [a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:results:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::results::chunk::0].",
    "citations": [
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:unknown:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::unknown::chunk::0",
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:results:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::results::chunk::0",
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:abstract:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::abstract::chunk::0",
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:methods:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::methods::chunk::0",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:unknown:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::unknown::chunk::0",
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:abstract:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::abstract::chunk::1",
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:conclusion:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::conclusion::chunk::0",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:references:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::references::chunk::14",
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:references:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::references::chunk::1",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:results:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::results::chunk::2"
    ]
  },
  "q07::strict": {
    "query": "How can C2LLM be deployed as a production code search engine?",
    "answer": "I cannot answer this with the information that I have at the moment.",
    "evidence": 1,
    "citations": []
  },
  "q14::strict": {
    "query": "How should governments regulate AI-driven productivity gains?",
    "answer": "The provided evidence does not contain information on how governments should regulate AI-driven productivity gains. The evidence focuses on quantifying the productivity gains from AI and decomposing the drivers of this progress.",
    "citations": [
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::11",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::4",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:references:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::references::chunk::0",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::3",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::6",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::1",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::5",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::1",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::9",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::9"
    ]
  },
  "q16::strict": {
    "query": "How does prediction noise differ from data noise in LLM benchmarks?",
    "answer": "Prediction noise in LLM benchmarks arises from a model generating different answers to the same question, whereas data noise stems from sampling questions [a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:abstract:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::abstract::chunk::0, a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:results:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::results::chunk::0]. Prediction noise is also referred to as prediction variance, which is due to the model generating different answers for a fixed question [a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:abstract:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::abstract::chunk::0]. Data noise, or data variance, measures how questions vary in difficulty and thus in their expected metrics [a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d32",
    "citations": [
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:unknown:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::unknown::chunk::0",
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:abstract:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::abstract::chunk::0",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::14",
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:results:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::results::chunk::0"
    ]
  },
  "q17::strict": {
    "query": "What is the law of total variance and how is it applied to LLM evals?",
    "answer": "The law of total variance states that for a random variable $A$, the total variance can be decomposed into the variance of the conditional expectation and the expectation of the conditional variance: $\\text{Var}[A] = \\text{Var}[\\mathbb{E}[A]] + \\mathbb{E}[\\text{Var}[A]]$ [a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed].\n\nIn the context of LLM evaluations, this law is applied to understand and quantify different sources of noise in the evaluation process [a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed]. Specifically, for a score $A(x, \\epsilon)$ where $x$ is a question and $\\epsilon$ represents noise or sampling, the total variance $\\text{Var}[A]$ can be broken down into:\n\n*   **Data Variance**: $\\text{Var}[\\mathbb{E}[A]]$ represents the variance due to the variability in the questions being asked. This is the variance of the expected score across different questions [a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed].\n*   **Prediction Variance**: $\\mathbb{E}[\\text{Var}[A]]$ represents the variance arising from the model's own variability or sampling noise for a given question. This is the expected variance of the scores for a fixed question [a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed].\n\nThis decomposition is useful for analyzing paired comparisons between two models, $A$ and $B$, where the law of total variance is applied to the difference $A-B$: $\\",
    "citations": [
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:methods:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::methods::chunk::2",
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:methods:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::methods::chunk::3",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:unknown:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::unknown::chunk::0",
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:unknown:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::unknown::chunk::0"
    ]
  },
  "q18::strict": {
    "query": "Why does paired analysis reduce data noise in model comparisons?",
    "answer": "Paired analysis reduces data noise in model comparisons by leveraging the correlation between model performances on the same questions. When comparing two models, A and B, the paired variance is calculated as Var[A(x) - B(x)] = Var[A] + Var[B] - 2Cov[A(x), B(x)] [a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:methods:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::methods::chunk::1]. The covariance term, Cov[A(x), B(x)], can significantly reduce the paired variance, especially when models A and B are highly correlated in their performance on the same questions [a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:methods:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::methods::chunk::1]. This reduction in variance, or noise, increases statistical power, allowing for the detection of smaller effects [a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:abstract:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8",
    "citations": [
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:abstract:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::abstract::chunk::0",
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:references:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::references::chunk::1",
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:methods:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::methods::chunk::1"
    ]
  },
  "q19::strict": {
    "query": "What is the all-pairs paired method proposed for LLM evaluation?",
    "answer": "The all-pairs paired method proposed for LLM evaluation involves comparing every possible pair of models on the same set of evaluation questions [a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:methods:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::methods::chunk::1]. This method aims to increase statistical power by accounting for the correlation between model predictions on the same question [a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:methods:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::methods::chunk::1]. The paired variance is calculated as Var[A(x) - B(x)] = Var[A(x)] + Var[B(x)] - 2Cov[A(x), B(x)], where the covariance term can reduce the paired variance if the models are correlated [a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:methods:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::methods::chunk::1]. This approach allows for the detection of smaller differences between models",
    "citations": [
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:methods:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::methods::chunk::1",
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:results:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::results::chunk::2",
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:references:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::references::chunk::0",
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:unknown:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::unknown::chunk::0",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:references:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::references::chunk::38",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:references:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::references::chunk::37",
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:references:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::references::chunk::1",
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:conclusion:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::conclusion::chunk::0",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:references:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::references::chunk::35",
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:results:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::results::chunk::3"
    ]
  },
  "q20::strict": {
    "query": "How can averaging reduce prediction noise without changing the mean?",
    "answer": "The provided evidence does not explain how averaging can reduce prediction noise without changing the mean.",
    "citations": [
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::9",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:references:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::references::chunk::0",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::10",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::7",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:references:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::references::chunk::31",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::0"
    ]
  },
  "q22::strict": {
    "query": "What challenges arise when merging fine-tuned models trained on heterogeneous tasks?",
    "answer": "Merging fine-tuned models trained on heterogeneous tasks presents challenges due to the fundamental mismatch between the diverse data distributions they were trained on [baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::0]. This heterogeneity can lead to brittle performance and high sensitivity to scaling initialization when current methods rely on heuristics to approximate optimal parameter combinations [baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::0]. Furthermore, these approaches do not explicitly account for the heterogeneity across tasks [baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::4]. The merging process typically forbids access to the original training data [baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::0].",
    "citations": [
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::0",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::5",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::8",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::4",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::9",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::11",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::10",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::20",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:references:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::references::chunk::0",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::6"
    ]
  },
  "q23::strict": {
    "query": "What is the cross-task heterogeneity term introduced in the PAC-Bayes bound?",
    "answer": "The cross-task heterogeneity term, denoted as $H_Q(\\alpha, \\beta)$, measures how much worse a fine-tuned model $Q_j$ performs on dataset $D_i$ compared to its own domain $D_j$. This term becomes zero if the datasets $D_i$ and $D_j$ are identical or if $Q_i \\equiv Q_j$ for all $j$, which is the case in a jointly trained multi-task learning model [baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::13]. This term is also described as capturing \"transfer mismatch across tasks\" [baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::15].",
    "citations": [
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::18",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::13",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::14",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::17",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:references:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::references::chunk::26",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::10",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::15",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::22",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::23",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::21"
    ]
  },
  "q24::strict": {
    "query": "How does SAMerging use Sharpness-Aware Minimization during merging?",
    "answer": "SAMerging integrates Sharpness-Aware Minimization (SAM) to explicitly navigate towards flat loss basins [baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::25]. This is motivated by the PAC-Bayes generalization bound, which suggests that favoring flatter basins, for example via sharpness-aware perturbations, can tighten the bound [baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::19]. SAMerging uses SAM to stabilize adaptation and mitigate cross-task KL interference [baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::28]. An ablation study showed that SAM, along with a KL objective, provides measurable gains, with their combination yielding the strongest improvements [baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f7642",
    "citations": [
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::24",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::29",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::28",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:references:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::references::chunk::33",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::25",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::8",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:references:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::references::chunk::4",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::1",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::19",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::26"
    ]
  },
  "q25::strict": {
    "query": "Why is KL-divergence minimization suitable for multi-teacher knowledge distillation?",
    "answer": "Minimizing the KL divergence between the student's (merged model) and the teachers' (fine-tuned models) distributions tightens a provable upper bound on the merged model's excess risk [baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::0]. This approach aligns with theoretical analysis and outperforms entropy-based merging criteria [baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::5]. Specifically, minimizing the KL divergence directly tightens the proven risk bound for the merged model [baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::23]. The objective achieves this by minimizing the student-teacher KL divergence on calibration data, thereby directly tightening the proven risk bound for the merged model [baf53702f26138946e51a878109ef0c655df6a9e17fff4ae4",
    "citations": [
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:unknown:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::unknown::chunk::0",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::20",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::23",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::5",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::0",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::9",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::10",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::30",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:references:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::references::chunk::25",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::3"
    ]
  },
  "q26::strict": {
    "query": "How does flatness relate to generalization in merged models?",
    "answer": "Flatness relates to generalization in merged models by contributing to better generalization and robustness [baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::9]. Specifically, flatter minima are linked to improved generalization and robustness [baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::3, baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::9]. In the context of multi-task learning (MTL), favoring flat basins can reduce cross-task sensitivity, enabling the merged model to generalize better [baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c6",
    "citations": [
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::0",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::19",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::20",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:references:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::references::chunk::6",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::9",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::12",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::4",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:references:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::references::chunk::7",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::10",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::3"
    ]
  },
  "q27::strict": {
    "query": "What empirical evidence shows SAMerging outperforming AdaMerging?",
    "answer": "The provided evidence does not contain information that shows SAMerging outperforming AdaMerging.",
    "citations": [
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:references:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::references::chunk::35",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::13",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::1",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::11",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::5",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::2",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::6",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:references:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::references::chunk::10",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:abstract:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::abstract::chunk::2",
      "173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be:results:173b0e88ded15dc1359a15f73ccbbecf489e94db6d8a293ff290dac3e9dc89be::sec::results::chunk::0"
    ]
  },
  "q29::strict": {
    "query": "Why does decoding order matter in masked diffusion models?",
    "answer": "Decoding order matters in masked diffusion models because it influences the model's cumulative predictive uncertainty along a generation path [baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::2]. This uncertainty, quantified by Denoising Entropy, reflects how confident the model is throughout the generation process [baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::2]. Different decoding strategies can lead to different expected uncertainties, and a single strategy can result in variance in path uncertainty [baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::2]. Furthermore, revealing additional context does not increase the prediction entropy, indicating that the decoding order influences the reduction of uncertainty [dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:methods:dd",
    "citations": [
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:unknown:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::unknown::chunk::0",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:references:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::references::chunk::0",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:methods:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::13",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:methods:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::14",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:methods:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::0",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:results:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::results::chunk::0",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:abstract:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::12",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:methods:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::1",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:abstract:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::abstract::chunk::2",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:references:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::references::chunk::8"
    ]
  },
  "q30::strict": {
    "query": "What is Path Uncertainty in the context of masked diffusion models?",
    "answer": "Path Uncertainty in the context of masked diffusion models refers to the model's cumulative predictive uncertainty along a single decoding path [dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:abstract:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::abstract::chunk::2]. It quantifies how confident the model is throughout the entire generation of a sequence [dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:abstract:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::abstract::chunk::2]. This uncertainty is measured by Path Entropy, which is the integral of State Entropy across the entire decoding path [dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:abstract:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::abstract::chunk::2].",
    "citations": [
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:unknown:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::unknown::chunk::0",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:abstract:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::abstract::chunk::2",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:methods:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::13",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:abstract:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::abstract::chunk::5",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:methods:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::14",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:methods:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::9",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:methods:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::11"
    ]
  },
  "q31::strict": {
    "query": "How is State Entropy (hDE) computed during decoding?",
    "answer": "The provided evidence does not contain information on how State Entropy (hDE) is computed during decoding.",
    "citations": [
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::4",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::abstract::chunk::3",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::2",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::18",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::abstract::chunk::2",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::6",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::14",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::references::chunk::0",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::unknown::chunk::0"
    ]
  },
  "q32::strict": {
    "query": "How does Path Entropy (HDE) summarize uncertainty across a decoding path?",
    "answer": "The provided evidence is insufficient to answer the question.",
    "citations": [
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::abstract::chunk::2",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::abstract::chunk::3",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::abstract::chunk::5",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::2",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::unknown::chunk::0",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::7",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::11",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::9",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::14",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::10"
    ]
  },
  "q33::strict": {
    "query": "What is the Entropy-based Best-of-N decoding strategy?",
    "answer": "The provided evidence is insufficient to answer the question about the Entropy-based Best-of-N decoding strategy.",
    "citations": [
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::abstract::chunk::3",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::results::chunk::3",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::15",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::4",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::2",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::unknown::chunk::0",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::18",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::references::chunk::0",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::5",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::6"
    ]
  },
  "q34::strict": {
    "query": "How does entropy-guided Sequential Monte Carlo improve generation quality?",
    "answer": "The provided evidence is insufficient to answer the question.",
    "citations": [
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::abstract::chunk::2",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::16",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::results::chunk::3",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::abstract::chunk::5",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::abstract::chunk::3",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::10",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::2",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::results::chunk::0",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::15",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::12"
    ]
  },
  "q21::strict": {
    "query": "How should leaderboard scores be recalibrated for public release?",
    "answer": "I cannot answer this with the information that I have at the moment.",
    "evidence": 0,
    "citations": []
  },
  "q28::strict": {
    "query": "How can SAMerging be adapted for continual learning systems?",
    "answer": "The provided evidence is insufficient to answer the question.",
    "citations": [
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:sec:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::28",
      "a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed:sec:a689e86d4eab17566942d2922d89ef5a79074b4d5e6d7d320c5f3507ab8801ed::sec::methods::chunk::0",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::abstract::chunk::1",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:sec:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::references::chunk::6",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:sec:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::3",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:sec:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::references::chunk::3",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:sec:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::abstract::chunk::25",
      "22aa53245f6ef08b6d5380a270e0461bcf35fe98232746c006ed9bc7edd3f7de:sec:22aa53245f6ef08b6d5380a270e0461bcf35fe98232746c006ed9bc7edd3f7de::sec::references::chunk::1",
      "baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5:sec:baf53702f26138946e51a878109ef0c655df6a9e17fff4ae45f76422bfcce3a5::sec::references::chunk::5",
      "22aa53245f6ef08b6d5380a270e0461bcf35fe98232746c006ed9bc7edd3f7de:sec:22aa53245f6ef08b6d5380a270e0461bcf35fe98232746c006ed9bc7edd3f7de::sec::references::chunk::0"
    ]
  },
  "q35::strict": {
    "query": "How can Denoising Entropy be applied to image diffusion models?",
    "answer": "The provided evidence does not contain information on how Denoising Entropy can be applied to image diffusion models.",
    "citations": [
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::unknown::chunk::0",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::2",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::abstract::chunk::2",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::references::chunk::0",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::14",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::10",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::abstract::chunk::5",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::11",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::results::chunk::0",
      "dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755:sec:dd30b2ec51ab363075995220df06f5f47e22702dfe454fc9be20ebfcb580e755::sec::methods::chunk::7"
    ]
  }
}